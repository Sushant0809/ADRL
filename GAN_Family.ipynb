{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2de47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:05.249942Z",
     "iopub.status.busy": "2023-10-31T16:53:05.249502Z",
     "iopub.status.idle": "2023-10-31T16:53:09.338311Z",
     "shell.execute_reply": "2023-10-31T16:53:09.337138Z"
    },
    "papermill": {
     "duration": 4.100622,
     "end_time": "2023-10-31T16:53:09.341004",
     "exception": false,
     "start_time": "2023-10-31T16:53:05.240382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfaabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:09.356526Z",
     "iopub.status.busy": "2023-10-31T16:53:09.356097Z",
     "iopub.status.idle": "2023-10-31T16:53:09.362925Z",
     "shell.execute_reply": "2023-10-31T16:53:09.361992Z"
    },
    "papermill": {
     "duration": 0.016613,
     "end_time": "2023-10-31T16:53:09.364951",
     "exception": false,
     "start_time": "2023-10-31T16:53:09.348338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    \"\"\"\n",
    "    Initializes the weights of the layer, w.\n",
    "    \"\"\"\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('conv') != -1:\n",
    "        nn.init.normal_(w.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(w.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(w.bias.data, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da8412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:09.380460Z",
     "iopub.status.busy": "2023-10-31T16:53:09.380116Z",
     "iopub.status.idle": "2023-10-31T16:53:09.390612Z",
     "shell.execute_reply": "2023-10-31T16:53:09.389806Z"
    },
    "papermill": {
     "duration": 0.020291,
     "end_time": "2023-10-31T16:53:09.392672",
     "exception": false,
     "start_time": "2023-10-31T16:53:09.372381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Generator, self).__init__()\n",
    "        nz, ngf, nc = params['nz'], params['ngf'], params['nc']\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Transposed Convolution Block 1\n",
    "            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Transposed Convolution Block 2\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Transposed Convolution Block 2\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Transposed Convolution Block 3\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Transposed Convolution Block 4\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Final Transposed Convolution Block\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef28d7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:09.407492Z",
     "iopub.status.busy": "2023-10-31T16:53:09.407176Z",
     "iopub.status.idle": "2023-10-31T16:53:09.417727Z",
     "shell.execute_reply": "2023-10-31T16:53:09.416876Z"
    },
    "papermill": {
     "duration": 0.020326,
     "end_time": "2023-10-31T16:53:09.419830",
     "exception": false,
     "start_time": "2023-10-31T16:53:09.399504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Discriminator, self).__init__()\n",
    "        nc, ndf = params['nc'], params['ndf']\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Convolution Block 1\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # Convolution Block 2\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # Convolution Block 3\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # Convolution Block 4\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Convolution Block 4\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # Final Convolution Layer\n",
    "            nn.Conv2d(ndf * 16, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fecba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:09.434801Z",
     "iopub.status.busy": "2023-10-31T16:53:09.434483Z",
     "iopub.status.idle": "2023-10-31T16:53:09.439834Z",
     "shell.execute_reply": "2023-10-31T16:53:09.438935Z"
    },
    "papermill": {
     "duration": 0.015428,
     "end_time": "2023-10-31T16:53:09.442074",
     "exception": false,
     "start_time": "2023-10-31T16:53:09.426646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters to define the model.\n",
    "params = {\n",
    "    \"bsize\" : 128,# Batch size during training.\n",
    "    'imsize' : 128,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 3,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 64,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 64, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 200,# Number of training epochs.\n",
    "    'lr' : 0.0002,# Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'save_epoch' : 2}# Save step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443531c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:09.456913Z",
     "iopub.status.busy": "2023-10-31T16:53:09.456630Z",
     "iopub.status.idle": "2023-10-31T16:53:09.532558Z",
     "shell.execute_reply": "2023-10-31T16:53:09.531533Z"
    },
    "papermill": {
     "duration": 0.085949,
     "end_time": "2023-10-31T16:53:09.534908",
     "exception": false,
     "start_time": "2023-10-31T16:53:09.448959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f567e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:09.550374Z",
     "iopub.status.busy": "2023-10-31T16:53:09.550053Z",
     "iopub.status.idle": "2023-10-31T16:53:29.459010Z",
     "shell.execute_reply": "2023-10-31T16:53:29.457937Z"
    },
    "papermill": {
     "duration": 19.919335,
     "end_time": "2023-10-31T16:53:29.461720",
     "exception": false,
     "start_time": "2023-10-31T16:53:09.542385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to your dataset and other hyperparameters\n",
    "data_dir_train = '/data4/home/anujkumar1/GAN/Datasets/images/train' \n",
    "data_dir_val = '/data4/home/anujkumar1/GAN/Datasets/images/val'\n",
    "shuffle_dataset = True  \n",
    "num_workers = 4  \n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((params['imsize'],params['imsize'])), \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root= data_dir_train, transform=transform)\n",
    "val_dataset = ImageFolder(root= data_dir_val , transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=params['bsize'],\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948b93b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:29.477484Z",
     "iopub.status.busy": "2023-10-31T16:53:29.477132Z",
     "iopub.status.idle": "2023-10-31T16:53:35.933728Z",
     "shell.execute_reply": "2023-10-31T16:53:35.932623Z"
    },
    "papermill": {
     "duration": 6.47698,
     "end_time": "2023-10-31T16:53:35.945915",
     "exception": false,
     "start_time": "2023-10-31T16:53:29.468935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training images.\n",
    "sample_batch = next(iter(val_dataloader))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(\n",
    "    sample_batch[0].to(device)[ : 64], padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ae8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:35.985612Z",
     "iopub.status.busy": "2023-10-31T16:53:35.985238Z",
     "iopub.status.idle": "2023-10-31T16:53:36.105133Z",
     "shell.execute_reply": "2023-10-31T16:53:36.104075Z"
    },
    "papermill": {
     "duration": 0.142949,
     "end_time": "2023-10-31T16:53:36.107690",
     "exception": false,
     "start_time": "2023-10-31T16:53:35.964741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the generator.\n",
    "netG = Generator(params).to(device)\n",
    "# Apply the weights_init() function to randomly initialize all\n",
    "# weights to mean=0.0, stddev=0.2\n",
    "netG.apply(weights_init)\n",
    "# Print the model.\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b20ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:36.147833Z",
     "iopub.status.busy": "2023-10-31T16:53:36.146898Z",
     "iopub.status.idle": "2023-10-31T16:53:36.265134Z",
     "shell.execute_reply": "2023-10-31T16:53:36.264094Z"
    },
    "papermill": {
     "duration": 0.141115,
     "end_time": "2023-10-31T16:53:36.267598",
     "exception": false,
     "start_time": "2023-10-31T16:53:36.126483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "netD = Discriminator(params).to(device)\n",
    "# Apply the weights_init() function to randomly initialize all\n",
    "# weights to mean=0.0, stddev=0.2\n",
    "netD.apply(weights_init)\n",
    "# Print the model.\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf66fec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:36.307537Z",
     "iopub.status.busy": "2023-10-31T16:53:36.307188Z",
     "iopub.status.idle": "2023-10-31T16:53:36.312040Z",
     "shell.execute_reply": "2023-10-31T16:53:36.310918Z"
    },
    "papermill": {
     "duration": 0.027657,
     "end_time": "2023-10-31T16:53:36.314284",
     "exception": false,
     "start_time": "2023-10-31T16:53:36.286627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Binary Cross Entropy loss function.\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c36fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:36.353922Z",
     "iopub.status.busy": "2023-10-31T16:53:36.352983Z",
     "iopub.status.idle": "2023-10-31T16:53:36.359656Z",
     "shell.execute_reply": "2023-10-31T16:53:36.358927Z"
    },
    "papermill": {
     "duration": 0.028473,
     "end_time": "2023-10-31T16:53:36.361848",
     "exception": false,
     "start_time": "2023-10-31T16:53:36.333375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
    "\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0236f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:36.401856Z",
     "iopub.status.busy": "2023-10-31T16:53:36.401527Z",
     "iopub.status.idle": "2023-10-31T16:53:36.407544Z",
     "shell.execute_reply": "2023-10-31T16:53:36.406752Z"
    },
    "papermill": {
     "duration": 0.028241,
     "end_time": "2023-10-31T16:53:36.409716",
     "exception": false,
     "start_time": "2023-10-31T16:53:36.381475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizer for the discriminator.\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "# Optimizer for the generator.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1472e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:36.449542Z",
     "iopub.status.busy": "2023-10-31T16:53:36.449197Z",
     "iopub.status.idle": "2023-10-31T16:53:36.453915Z",
     "shell.execute_reply": "2023-10-31T16:53:36.452867Z"
    },
    "papermill": {
     "duration": 0.028029,
     "end_time": "2023-10-31T16:53:36.455959",
     "exception": false,
     "start_time": "2023-10-31T16:53:36.427930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "# Stores generator losses during training.\n",
    "G_losses = []\n",
    "# Stores discriminator losses during training.\n",
    "D_losses = []\n",
    "\n",
    "iters = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78deff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T16:53:36.495715Z",
     "iopub.status.busy": "2023-10-31T16:53:36.495348Z",
     "iopub.status.idle": "2023-10-31T19:19:26.694372Z",
     "shell.execute_reply": "2023-10-31T19:19:26.693108Z"
    },
    "papermill": {
     "duration": 8750.241359,
     "end_time": "2023-10-31T19:19:26.715872",
     "exception": false,
     "start_time": "2023-10-31T16:53:36.474513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize lists to track generator and discriminator losses\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "# Initialize a list to store generated images for visualization\n",
    "img_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(params['nepochs']):\n",
    "    data_iterator = tqdm(enumerate(train_dataloader, 0), total=len(train_dataloader), desc=f\"Epoch {epoch+1}\")\n",
    "    for i, data in data_iterator:\n",
    "        # Transfer data tensor to GPU/CPU (device)\n",
    "        real_data = data[0].to(device)\n",
    "        b_size = real_data.size(0)\n",
    "\n",
    "        # Discriminator training\n",
    "        netD.zero_grad()\n",
    "        label = torch.full((b_size, ), real_label, device=device)\n",
    "        output = netD(real_data).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Generator training\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake_data).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        \n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Visualize generated images\n",
    "        if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(train_dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_data = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "        data_iterator.set_description(f\"Epoch {epoch+1}, Loss_D: {errD.item():.4f}, Loss_G: {errG.item():.4f}\")\n",
    "    # Check progress\n",
    "        # if i % 50 == 0:\n",
    "    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "            % (epoch+1, params['nepochs'], i, len(train_dataloader),\n",
    "                errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "    \n",
    "    if (epoch+1)%50 == 0:\n",
    "        noise = torch.randn(100, params['nz'], 1, 1, device=device)\n",
    "        # Generate images\n",
    "        with torch.no_grad():\n",
    "            generated_imgs = netG(noise).detach().cpu()\n",
    "\n",
    "        # Reshape the generated images into a 10x10 grid\n",
    "        grid = vutils.make_grid(generated_imgs, nrow= 10, padding=2, normalize=True)\n",
    "\n",
    "        # Display the generated image grid with a larger size\n",
    "        plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Generated Images\")\n",
    "        plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d661c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T19:19:35.095382Z",
     "iopub.status.busy": "2023-10-31T19:19:35.094960Z",
     "iopub.status.idle": "2023-10-31T19:19:35.651485Z",
     "shell.execute_reply": "2023-10-31T19:19:35.650543Z"
    },
    "papermill": {
     "duration": 4.759428,
     "end_time": "2023-10-31T19:19:35.653759",
     "exception": false,
     "start_time": "2023-10-31T19:19:30.894331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training losses.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b31706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the Generator model\n",
    "# torch.save(netG.state_dict(), 'DCGAN_generator_model.pth')\n",
    "\n",
    "# # Save the Discriminator model\n",
    "# torch.save(netD.state_dict(), 'DCGAN_discriminator_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bab08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T19:20:16.067025Z",
     "iopub.status.busy": "2023-10-31T19:20:16.066268Z",
     "iopub.status.idle": "2023-10-31T19:20:17.230130Z",
     "shell.execute_reply": "2023-10-31T19:20:17.229132Z"
    },
    "papermill": {
     "duration": 5.266472,
     "end_time": "2023-10-31T19:20:17.253715",
     "exception": false,
     "start_time": "2023-10-31T19:20:11.987243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = torch.randn(100, params['nz'], 1, 1, device=device)\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    generated_imgs = netG(noise).detach().cpu()\n",
    "\n",
    "# Reshape the generated images into a 10x10 grid\n",
    "grid = vutils.make_grid(generated_imgs, nrow= 10, padding=2, normalize=True)\n",
    "\n",
    "# Display the generated image grid with a larger size\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795f160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T19:20:25.797412Z",
     "iopub.status.busy": "2023-10-31T19:20:25.796496Z",
     "iopub.status.idle": "2023-10-31T19:20:29.725122Z",
     "shell.execute_reply": "2023-10-31T19:20:29.723892Z"
    },
    "papermill": {
     "duration": 8.162075,
     "end_time": "2023-10-31T19:20:29.747286",
     "exception": false,
     "start_time": "2023-10-31T19:20:21.585211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = torch.randn(100, params['nz'], 1, 1, device=device)\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    generated_imgs = netG(noise).detach().cpu()\n",
    "\n",
    "# Reshape the generated images into a 10x10 grid\n",
    "grid = vutils.make_grid(generated_imgs, nrow=10, padding=2, normalize=True)\n",
    "\n",
    "# Plot the training images.\n",
    "sample_batch = next(iter(val_dataloader))\n",
    "\n",
    "# Create subplots for generated images and training images side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Display the training images\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"Real Images\")\n",
    "axes[0].imshow(np.transpose(vutils.make_grid(\n",
    "    sample_batch[0].to(device)[:100],nrow=10, padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "# Display the generated image grid\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Generated Images\")\n",
    "axes[1].imshow(np.transpose(grid, (1, 2, 0)))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697828b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T19:20:38.294429Z",
     "iopub.status.busy": "2023-10-31T19:20:38.294016Z",
     "iopub.status.idle": "2023-10-31T19:20:39.514848Z",
     "shell.execute_reply": "2023-10-31T19:20:39.513777Z"
    },
    "papermill": {
     "duration": 5.437924,
     "end_time": "2023-10-31T19:20:39.517614",
     "exception": false,
     "start_time": "2023-10-31T19:20:34.079690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Load a pre-trained Inception model\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "# Load the Inception-V3 model without using deprecated parameters\n",
    "inception_model = inception_v3(pretrained=True, num_classes=1000)\n",
    "inception_model.aux_logits = False  # Disable auxiliary logits\n",
    "inception_model.fc = nn.Identity()  # Remove the final classification layer\n",
    "\n",
    "def extract_inception_features(images, batch_size=32):\n",
    "    transform = transforms.Compose([transforms.Resize((299, 299))])\n",
    "    dataset = torch.utils.data.TensorDataset(images, torch.zeros(len(images)))  # Create a dummy target\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    inception_features = []\n",
    "\n",
    "    inception_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch_images, _ = batch\n",
    "            batch_images = transform(batch_images)  # Resize the images to 299x299\n",
    "            batch_features = inception_model(batch_images)\n",
    "            inception_features.append(batch_features)\n",
    "\n",
    "    inception_features = torch.cat(inception_features, 0)\n",
    "    return inception_features.numpy()\n",
    "\n",
    "\n",
    "# Function to calculate FID\n",
    "def calculate_fid(real_features, gen_features):\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    mu_gen = np.mean(gen_features, axis=0)\n",
    "    \n",
    "    cov_real = np.cov(real_features, rowvar=False)\n",
    "    cov_gen = np.cov(gen_features, rowvar=False)\n",
    "    \n",
    "    cov_sqrt = sqrtm(np.dot(cov_real, cov_gen))\n",
    "    \n",
    "    fid = np.sum((mu_real - mu_gen) ** 2) + np.trace(cov_real + cov_gen - 2 * cov_sqrt)\n",
    "    \n",
    "    return fid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a9267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T19:20:47.965864Z",
     "iopub.status.busy": "2023-10-31T19:20:47.965103Z",
     "iopub.status.idle": "2023-10-31T19:20:51.091462Z",
     "shell.execute_reply": "2023-10-31T19:20:51.090057Z"
    },
    "papermill": {
     "duration": 7.329724,
     "end_time": "2023-10-31T19:20:51.094170",
     "exception": false,
     "start_time": "2023-10-31T19:20:43.764446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = torch.randn(1000, params['nz'], 1, 1, device=device)\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    generated_imgs = netG(noise).detach().cpu()\n",
    "    \n",
    "real = []\n",
    "real_labels = []\n",
    "\n",
    "for batch_images, batch_labels in val_dataloader:\n",
    "    # Append the batch of images and labels to the collected lists\n",
    "    real.append(batch_images)\n",
    "    real_labels.append(batch_labels)\n",
    "\n",
    "    # Check if you have collected enough images\n",
    "    num_collected_images = sum(len(images) for images in real)\n",
    "    if num_collected_images >= 1000:\n",
    "        break\n",
    "\n",
    "\n",
    "real_imgs = torch.cat(real)[:1000]\n",
    "real_labels = torch.cat(real_labels)[:1000]\n",
    "real_imgs = real_imgs.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8e7bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T19:20:59.732818Z",
     "iopub.status.busy": "2023-10-31T19:20:59.732391Z",
     "iopub.status.idle": "2023-10-31T19:23:41.314602Z",
     "shell.execute_reply": "2023-10-31T19:23:41.313035Z"
    },
    "papermill": {
     "duration": 170.205778,
     "end_time": "2023-10-31T19:23:45.645491",
     "exception": false,
     "start_time": "2023-10-31T19:20:55.439713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming real_images and gen_images are PyTorch tensors or numpy arrays\n",
    "# Extract features from real and generated images\n",
    "real_features = extract_inception_features(real_imgs)\n",
    "gen_features = extract_inception_features(generated_imgs)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(real_features, gen_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ace4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FID: {np.real(fid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Training Setup:**\n",
    "   - DCGAN trained on provided data with standard GAN loss.\n",
    "   - Varying training iterations for both generator and discriminator.\n",
    "\n",
    "2. **Loss Analysis:**\n",
    "   - Studied loss curves for generator and discriminator.\n",
    "   - Observed convergence patterns and behaviors during training.\n",
    "\n",
    "3. **Generated Images:**\n",
    "   - Presented a 10x10 grid of generated images.\n",
    "   - Noted any distinctive features or patterns in the images.\n",
    "\n",
    "4. **FID Score:**\n",
    "   - Calculated FID score of 48.00345601901948.\n",
    "   - FID used to assess likeness between real and generated images.\n",
    "\n",
    "5. **Observations:**\n",
    "   - Varied training iterations impacted model performance.\n",
    "   - Reflect on the implications of FID score in evaluating image quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0dca53",
   "metadata": {},
   "source": [
    "#### Multiple number of Discriminator Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba565ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = [2,3,5]   \n",
    "\n",
    "for iter in num_iter:\n",
    "    # Create the generator.\n",
    "    netG = Generator(params).to(device)\n",
    "    # Apply the weights_init() function to randomly initialize all\n",
    "    # weights to mean=0.0, stddev=0.2\n",
    "    netG.apply(weights_init)\n",
    "    # Create the discriminator.\n",
    "    netD = Discriminator(params).to(device)\n",
    "    # Apply the weights_init() function to randomly initialize all\n",
    "    # weights to mean=0.0, stddev=0.2\n",
    "    netD.apply(weights_init)\n",
    "    # Binary Cross Entropy loss function.\n",
    "    criterion = nn.BCELoss()\n",
    "    fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
    "\n",
    "    real_label = 1.0\n",
    "    fake_label = 0.0\n",
    "\n",
    "    # Optimizer for the discriminator.\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "    # Optimizer for the generator.\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "\n",
    "    # Initialize lists to track generator and discriminator losses\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    iters = 0\n",
    "    # Initialize a list to store generated images for visualization\n",
    "    img_list = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(params['nepochs']):\n",
    "        # data_iterator = tqdm(enumerate(train_dataloader, 0), total=len(train_dataloader), desc=f\"Epoch {epoch+1}\")\n",
    "        for  data,_ in train_dataloader:\n",
    "            # Transfer data tensor to GPU/CPU (device)\n",
    "            real_data = data.to(device)\n",
    "            b_size = real_data.size(0)\n",
    "\n",
    "            for _ in range(iter):    \n",
    "                # Discriminator training\n",
    "                netD.zero_grad()\n",
    "                label = torch.full((b_size, ), real_label, device=device)\n",
    "                output = netD(real_data).view(-1)\n",
    "                errD_real = criterion(output, label)\n",
    "                errD_real.backward()\n",
    "                D_x = output.mean().item()\n",
    "\n",
    "                noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "                fake_data = netG(noise)\n",
    "                label.fill_(fake_label)\n",
    "                output = netD(fake_data.detach()).view(-1)\n",
    "                errD_fake = criterion(output, label)\n",
    "                errD_fake.backward()\n",
    "                D_G_z1 = output.mean().item()\n",
    "\n",
    "                errD = errD_real + errD_fake\n",
    "                optimizerD.step()\n",
    "\n",
    "            # Generator training\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake_data).view(-1)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "            \n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "\n",
    "            # # Visualize generated images\n",
    "            # if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(train_dataloader)-1)):\n",
    "            #     with torch.no_grad():\n",
    "            #         fake_data = netG(fixed_noise).detach().cpu()\n",
    "            #     img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
    "\n",
    "            iters += 1\n",
    "            \n",
    "            # data_iterator.set_description(f\"Epoch {epoch+1}, Loss_D: {errD.item():.4f}, Loss_G: {errG.item():.4f}\")\n",
    "        # # Check progress\n",
    "        # if (epoch+1) % 10 == 0:\n",
    "        #     print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "        #             % (epoch+1, params['nepochs'], len(train_dataloader),\n",
    "        #                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            \n",
    "        if (epoch+1)%50 == 0:\n",
    "            noise = torch.randn(100, params['nz'], 1, 1, device=device)\n",
    "            # Generate images\n",
    "            with torch.no_grad():\n",
    "                generated_imgs = netG(noise).detach().cpu()\n",
    "\n",
    "            # Reshape the generated images into a 10x10 grid\n",
    "            grid = vutils.make_grid(generated_imgs, nrow= 10, padding=2, normalize=True)\n",
    "\n",
    "            # Display the generated image grid with a larger size\n",
    "            plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Generated Images (iter {iter})\")\n",
    "            plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    # Plot the training losses.\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(f\"Generator and Discriminator Loss During Training (iter {iter})\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the Generator model\n",
    "    torch.save(netG.state_dict(), f'DCGAN_generator_model_{iter}.pth')\n",
    "\n",
    "    # Save the Discriminator model\n",
    "    torch.save(netD.state_dict(), f'DCGAN_discriminator_model_{iter}.pth')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in num_iter:\n",
    "    # Load the Generator model\n",
    "    dis_path  = f'/data4/home/anujkumar1/GAN/DCGAN_discriminator_model_{iter}.pth'\n",
    "    gen_path = f'/data4/home/anujkumar1/GAN/DCGAN_generator_model_{iter}.pth'\n",
    "    netG.load_state_dict(torch.load(gen_path))\n",
    "    netG.eval()\n",
    "\n",
    "    # Load the Discriminator model\n",
    "    netD.load_state_dict(torch.load(dis_path))\n",
    "    netD.eval()\n",
    "\n",
    "        \n",
    "\n",
    "    noise = torch.randn(100, params['nz'], 1, 1, device=device)\n",
    "    # Generate images\n",
    "    with torch.no_grad():\n",
    "        generated_imgs = netG(noise).detach().cpu()\n",
    "\n",
    "    # Reshape the generated images into a 10x10 grid\n",
    "    grid = vutils.make_grid(generated_imgs, nrow= 10, padding=2, normalize=True)\n",
    "\n",
    "    # Display the generated image grid with a larger size\n",
    "    plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Generated Images (iter {iter})\")\n",
    "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "    noise = torch.randn(1000, params['nz'], 1, 1, device=device)\n",
    "    # Generate images\n",
    "    with torch.no_grad():\n",
    "        generated_imgs = netG(noise).detach().cpu()\n",
    "        \n",
    "    real = []\n",
    "    real_labels = []\n",
    "\n",
    "    for batch_images, batch_labels in val_dataloader:\n",
    "        # Append the batch of images and labels to the collected lists\n",
    "        real.append(batch_images)\n",
    "        real_labels.append(batch_labels)\n",
    "\n",
    "        # Check if you have collected enough images\n",
    "        num_collected_images = sum(len(images) for images in real)\n",
    "        if num_collected_images >= 1000:\n",
    "            break\n",
    "\n",
    "\n",
    "    real_imgs = torch.cat(real)[:1000]\n",
    "    real_labels = torch.cat(real_labels)[:1000]\n",
    "    real_imgs = real_imgs.detach().cpu()\n",
    "\n",
    "    # Assuming real_images and gen_images are PyTorch tensors or numpy arrays\n",
    "    # Extract features from real and generated images\n",
    "    real_features = extract_inception_features(real_imgs)\n",
    "    gen_features = extract_inception_features(generated_imgs)\n",
    "\n",
    "    # Calculate FID\n",
    "    fid = calculate_fid(real_features, gen_features)\n",
    "\n",
    "    print(f\"FID (iter {iter}): {np.real(fid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Training Iterations Impact:**\n",
    "   - Generator and discriminator trained 2, 3, and 5 times independently.\n",
    "   - Observed behavior changes in loss convergence and image generation.\n",
    "\n",
    "2. **FID Scores Analysis:**\n",
    "   - FID (iter 2): 68.0875, FID (iter 3): 92.4330, FID (iter 5): 68.4888.\n",
    "   - Notable fluctuations in FID scores with varying training iterations.\n",
    "\n",
    "3. **Behavioral Changes:**\n",
    "   - Increased training iterations led to improved stability (or degradation) in convergence.\n",
    "   - Analyzed the impact on the visual quality of generated images.\n",
    "   - There is repetion of image is observed when we increased the training of discriminator\n",
    "\n",
    "4. **Consideration for Optimal Iterations:**\n",
    "   - Evaluate trade-offs between training efficiency and image fidelity.\n",
    "   - Discuss implications for selecting the optimal number of iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d9123",
   "metadata": {},
   "source": [
    "### CWGAN with LABEL as INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df644a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.247986Z",
     "iopub.status.busy": "2023-11-04T13:00:27.247052Z",
     "iopub.status.idle": "2023-11-04T13:00:27.254533Z",
     "shell.execute_reply": "2023-11-04T13:00:27.253427Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.247947Z"
    },
    "papermill": {
     "duration": 13.807954,
     "end_time": "2023-11-03T22:05:17.432768",
     "exception": false,
     "start_time": "2023-11-03T22:05:03.624814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transformations\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import PIL.Image as Image\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ignite.metrics.gan import FID\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fd10c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.266351Z",
     "iopub.status.busy": "2023-11-04T13:00:27.266044Z",
     "iopub.status.idle": "2023-11-04T13:00:27.273430Z",
     "shell.execute_reply": "2023-11-04T13:00:27.272588Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.266324Z"
    },
    "papermill": {
     "duration": 0.016344,
     "end_time": "2023-11-03T22:05:17.455848",
     "exception": false,
     "start_time": "2023-11-03T22:05:17.439504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters to define the model.\n",
    "params = {\n",
    "    \"bsize\" : 64,# Batch size during training.\n",
    "    'imsize' : 128,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 3,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 64,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 64, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 200,# Number of training epochs.\n",
    "    'lr' : 1e-4,# Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'num_classes' : 3,# number of label in animal dataset\n",
    "    'gen_embs' : 100, # embedding of labels\n",
    "    'critic_iter' : 5, # number of critic iteration\n",
    "    'lambda_gp' : 10 # Gradient penality\n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4a2ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.275441Z",
     "iopub.status.busy": "2023-11-04T13:00:27.275125Z",
     "iopub.status.idle": "2023-11-04T13:00:27.285315Z",
     "shell.execute_reply": "2023-11-04T13:00:27.284312Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.275415Z"
    },
    "papermill": {
     "duration": 0.079905,
     "end_time": "2023-11-03T22:05:17.542060",
     "exception": false,
     "start_time": "2023-11-03T22:05:17.462155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1c9f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.287234Z",
     "iopub.status.busy": "2023-11-04T13:00:27.286542Z",
     "iopub.status.idle": "2023-11-04T13:00:27.396858Z",
     "shell.execute_reply": "2023-11-04T13:00:27.396019Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.287201Z"
    },
    "papermill": {
     "duration": 17.084415,
     "end_time": "2023-11-03T22:05:34.632920",
     "exception": false,
     "start_time": "2023-11-03T22:05:17.548505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to your dataset and other hyperparameters\n",
    "data_dir_train = '/data4/home/anujkumar1/GAN/Datasets/images/train' \n",
    "data_dir_val = '/data4/home/anujkumar1/GAN/Datasets/images/val'\n",
    "shuffle_dataset = True  \n",
    "num_workers = 4  \n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((params['imsize'],params['imsize'])), \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root= data_dir_train, transform=transform)\n",
    "val_dataset = ImageFolder(root= data_dir_val , transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=params['bsize'],\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60264372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.398283Z",
     "iopub.status.busy": "2023-11-04T13:00:27.397943Z",
     "iopub.status.idle": "2023-11-04T13:00:27.409768Z",
     "shell.execute_reply": "2023-11-04T13:00:27.408803Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.398250Z"
    },
    "papermill": {
     "duration": 0.02058,
     "end_time": "2023-11-03T22:05:34.659742",
     "exception": false,
     "start_time": "2023-11-03T22:05:34.639162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = params['imsize']\n",
    "        self.num_classes = params['num_classes']\n",
    "        self.embed_size = params['gen_embs']\n",
    "        nz = params['nz']\n",
    "        ngf = params['ngf']\n",
    "        nc = params['nc']\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            nn.ConvTranspose2d(nz + self.embed_size, ngf * 32, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 32),\n",
    "            nn.ReLU(),\n",
    "             # img: 4x4\n",
    "            nn.ConvTranspose2d(ngf * 32, ngf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(),\n",
    "            # img: 8x8\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(),\n",
    "            # img: 16x16\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(),\n",
    "            # img: 32x32\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(),\n",
    "            # img: 64x64\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, kernel_size=4, stride=2, padding=1),\n",
    "            # Output: N x channels_img x 128 x 128\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], 1)\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e54119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.412318Z",
     "iopub.status.busy": "2023-11-04T13:00:27.411974Z",
     "iopub.status.idle": "2023-11-04T13:00:27.425596Z",
     "shell.execute_reply": "2023-11-04T13:00:27.424718Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.412285Z"
    },
    "papermill": {
     "duration": 0.020673,
     "end_time": "2023-11-03T22:05:34.686449",
     "exception": false,
     "start_time": "2023-11-03T22:05:34.665776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = params['imsize']\n",
    "        nc = params['nc']\n",
    "        ndf = params['ndf']\n",
    "        num_classes = params['num_classes']\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc + 1, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Block 1\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "             # Block 4\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 16, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Output\n",
    "            nn.Conv2d(ndf * 16, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "        # Embedding for conditioning\n",
    "        self.embed = nn.Embedding(num_classes, self.img_size * self.img_size)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        return self.main(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79b49b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.433909Z",
     "iopub.status.busy": "2023-11-04T13:00:27.433636Z",
     "iopub.status.idle": "2023-11-04T13:00:27.441909Z",
     "shell.execute_reply": "2023-11-04T13:00:27.441043Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.433885Z"
    },
    "papermill": {
     "duration": 0.015919,
     "end_time": "2023-11-03T22:05:34.708597",
     "exception": false,
     "start_time": "2023-11-03T22:05:34.692678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    \"\"\"\n",
    "    Initializes the weights of the layer, w.\n",
    "    \"\"\"\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('conv') != -1:\n",
    "        nn.init.normal_(w.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('bn') != -1:\n",
    "        nn.init.normal_(w.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(w.bias.data, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e53730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.444037Z",
     "iopub.status.busy": "2023-11-04T13:00:27.443738Z",
     "iopub.status.idle": "2023-11-04T13:00:27.975872Z",
     "shell.execute_reply": "2023-11-04T13:00:27.974943Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.444001Z"
    },
    "papermill": {
     "duration": 6.153807,
     "end_time": "2023-11-03T22:05:40.868718",
     "exception": false,
     "start_time": "2023-11-03T22:05:34.714911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the generator model with the provided parameters.\n",
    "gen = Generator(params).to(device)\n",
    "gen.apply(weights_init)\n",
    "critic = Discriminator(params).to(device)\n",
    "critic.apply(weights_init), gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a8fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.977174Z",
     "iopub.status.busy": "2023-11-04T13:00:27.976903Z",
     "iopub.status.idle": "2023-11-04T13:00:27.983271Z",
     "shell.execute_reply": "2023-11-04T13:00:27.982413Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.977150Z"
    },
    "papermill": {
     "duration": 0.015785,
     "end_time": "2023-11-03T22:05:40.891043",
     "exception": false,
     "start_time": "2023-11-03T22:05:40.875258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initializate optimizer\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce96d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.984678Z",
     "iopub.status.busy": "2023-11-04T13:00:27.984436Z",
     "iopub.status.idle": "2023-11-04T13:00:27.994535Z",
     "shell.execute_reply": "2023-11-04T13:00:27.993820Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.984656Z"
    },
    "papermill": {
     "duration": 0.017003,
     "end_time": "2023-11-03T22:05:40.914799",
     "exception": false,
     "start_time": "2023-11-03T22:05:40.897796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(100, params['nz'], 1, 1).to(device)\n",
    "fixed_labels = torch.randint(0, 3, (100,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c75caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:28.006719Z",
     "iopub.status.busy": "2023-11-04T13:00:28.006454Z",
     "iopub.status.idle": "2023-11-04T13:00:28.342512Z",
     "shell.execute_reply": "2023-11-04T13:00:28.341657Z",
     "shell.execute_reply.started": "2023-11-04T13:00:28.006686Z"
    },
    "papermill": {
     "duration": 1.092895,
     "end_time": "2023-11-03T22:05:42.038519",
     "exception": false,
     "start_time": "2023-11-03T22:05:40.945624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### initialize FID wrapper\n",
    "fid_score = FID()\n",
    "#interpolate function to resize images to 299,299,3  which is the input size of inception network\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transformations.ToPILImage()(img)\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        arr.append(transformations.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121742ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Generator model\n",
    "gen.load_state_dict(torch.load('/data4/home/anujkumar1/GAN/generator_model.pth'))\n",
    "# gen.eval()\n",
    "\n",
    "# Load the Discriminator model\n",
    "critic.load_state_dict(torch.load('/data4/home/anujkumar1/GAN/critic_model.pth'))\n",
    "# critic.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc4271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:28.344102Z",
     "iopub.status.busy": "2023-11-04T13:00:28.343755Z",
     "iopub.status.idle": "2023-11-04T13:00:28.352117Z",
     "shell.execute_reply": "2023-11-04T13:00:28.351081Z",
     "shell.execute_reply.started": "2023-11-04T13:00:28.344074Z"
    },
    "papermill": {
     "duration": 0.019011,
     "end_time": "2023-11-03T22:05:42.065378",
     "exception": false,
     "start_time": "2023-11-03T22:05:42.046367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen.train()\n",
    "critic.train()\n",
    "#### gradient penalty function for WGAN-GP\n",
    "def gradient_penalty(critic,labels, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, labels)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57bf62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:28.353859Z",
     "iopub.status.busy": "2023-11-04T13:00:28.353546Z",
     "iopub.status.idle": "2023-11-04T13:00:29.783227Z",
     "shell.execute_reply": "2023-11-04T13:00:29.781714Z",
     "shell.execute_reply.started": "2023-11-04T13:00:28.353833Z"
    },
    "papermill": {
     "duration": 26704.314486,
     "end_time": "2023-11-04T05:30:46.386881",
     "exception": false,
     "start_time": "2023-11-03T22:05:42.072395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "G_loss = []\n",
    "C_loss = []\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    #we will track the total loss of the generator and critic for each epoch over the entire dataset\n",
    "    #initialize the total loss of the generator and critic for each epoch to 0\n",
    "    total_loss_gen = 0\n",
    "    total_loss_critic = 0\n",
    "    \n",
    "    #move these to device\n",
    "\n",
    "    #have no gradient for these losse\n",
    "    # total_loss_gen = torch.no_grad()\n",
    "    # total_loss_critic = torch.no_grad()\n",
    "    \n",
    "    # Target labels \n",
    "    for batch_idx, (real, labels) in enumerate(tqdm(train_dataloader)):\n",
    "        #send labels to device\n",
    "        labels = labels.to(device)\n",
    "        batch_step = 0\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        \n",
    "        \n",
    "\n",
    "        # Train Critic: \n",
    "        for _ in range(params['critic_iter']):\n",
    "            noise = torch.randn(cur_batch_size, params['nz'], 1, 1).to(device)\n",
    "            if len(noise) != len(labels):\n",
    "                noise = noise[:len(labels)]\n",
    "            fake = gen(noise, labels)\n",
    "            critic_real = critic(real, labels).reshape(-1)\n",
    "            critic_fake = critic(fake, labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic, labels, real, fake, device=device)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + params['lambda_gp'] * gp\n",
    "            )\n",
    "            \n",
    "            \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "            \n",
    "        #trained critic\n",
    "        \n",
    "\n",
    "        # Train Generator: \n",
    "        gen_fake = critic(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #update the total loss of the generator and critic for each batch in the epoch\n",
    "        #add just the value no gradients\n",
    "        #have no gradient for these losse\n",
    "        #add just the value no gradientsfrom the loss_gen tensor\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            total_loss_gen += loss_gen.item()\n",
    "            total_loss_critic += loss_critic.item()\n",
    "        \n",
    "        \n",
    "        # # Print losses occasionally and print to tensorboard in a batch\n",
    "        # if (batch_idx + 1) % 23 == 0 and batch_idx > 0:\n",
    "        #     print(\n",
    "        #         f\"Epoch [{epoch+1}/{params['nepochs']}] Batch {batch_idx+1}/{len(train_dataloader)} \\\n",
    "        #           Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "        #     )\n",
    "            \n",
    "        batch_step += 1\n",
    "    \n",
    "        G_loss.append(loss_gen.item())\n",
    "        C_loss.append(loss_critic.item())   \n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{params['nepochs']}] Batch {batch_idx+1}/{len(train_dataloader)} \\\n",
    "                Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "    if (epoch+1)% 20 == 0:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake = gen(fixed_noise, fixed_labels)\n",
    "            # take out (up to) 32 examples\n",
    "            grid = vutils.make_grid(fake, nrow= 10, padding=2, normalize=True)[:100].cpu().numpy()\n",
    "            # Display the generated image grid with a larger size\n",
    "            plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Generated Images\")\n",
    "            plt.imshow(np.transpose(grid[:100], (1, 2, 0)))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        #AVERAGE LOSS---\n",
    "\n",
    "        #get average loss of generator and critic for each epoch\n",
    "        avg_loss_gen = total_loss_gen / len(train_dataloader)\n",
    "        avg_loss_critic = total_loss_critic / len(train_dataloader)\n",
    "\n",
    "    \n",
    "    step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c48643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training losses.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and critic Loss During Training\")\n",
    "plt.plot(G_loss,label=\"G\")\n",
    "plt.plot(C_loss,label=\"C\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Generator model\n",
    "torch.save(gen.state_dict(), 'generator_model.pth')\n",
    "\n",
    "# Save the Discriminator model\n",
    "torch.save(critic.state_dict(), 'critic_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe407b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-04T13:00:29.784242Z",
     "iopub.status.idle": "2023-11-04T13:00:29.784615Z",
     "shell.execute_reply": "2023-11-04T13:00:29.784459Z",
     "shell.execute_reply.started": "2023-11-04T13:00:29.784441Z"
    },
    "papermill": {
     "duration": 0.926365,
     "end_time": "2023-11-04T05:30:48.176444",
     "exception": false,
     "start_time": "2023-11-04T05:30:47.250079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training losses.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and critic Loss During Training\")\n",
    "plt.plot(G_loss,label=\"G\")\n",
    "plt.plot(C_loss,label=\"C\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the Generator model\n",
    "# torch.save(gen.state_dict(), 'generator_model.pth')\n",
    "\n",
    "# # Save the Discriminator model\n",
    "# torch.save(critic.state_dict(), 'critic_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Generator model\n",
    "# gen.load_state_dict(torch.load('/data4/home/anujkumar1/GAN/generator_model.pth'))\n",
    "gen.eval()\n",
    "\n",
    "# Load the Discriminator model\n",
    "# critic.load_state_dict(torch.load('/data4/home/anujkumar1/GAN/critic_model.pth'))\n",
    "critic.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135580a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-04T13:00:29.786063Z",
     "iopub.status.idle": "2023-11-04T13:00:29.786530Z",
     "shell.execute_reply": "2023-11-04T13:00:29.786306Z",
     "shell.execute_reply.started": "2023-11-04T13:00:29.786285Z"
    }
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(100, params['nz'], 1, 1).to(device)\n",
    "fixed_labels_0  = torch.full((100,), 0).to(device)\n",
    "fixed_labels_1  = torch.full((100,), 1).to(device)\n",
    "fixed_labels_2  = torch.full((100,), 2).to(device)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    fake_0 = gen(fixed_noise, fixed_labels_0)\n",
    "    fake_1 = gen(fixed_noise, fixed_labels_1)\n",
    "    fake_2 = gen(fixed_noise, fixed_labels_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e562e7e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-04T13:00:29.787537Z",
     "iopub.status.idle": "2023-11-04T13:00:29.787963Z",
     "shell.execute_reply": "2023-11-04T13:00:29.787764Z",
     "shell.execute_reply.started": "2023-11-04T13:00:29.787743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape the generated images into a 10x10 grid\n",
    "grid_0 = vutils.make_grid(fake_0, nrow= 10, padding=2, normalize=True)[:100].cpu().numpy()\n",
    "grid_1 = vutils.make_grid(fake_1, nrow= 10, padding=2, normalize=True)[:100].cpu().numpy()\n",
    "grid_2 = vutils.make_grid(fake_2, nrow= 10, padding=2, normalize=True)[:100].cpu().numpy()\n",
    "\n",
    "# Display the generated image grid with a larger size\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images_0\")\n",
    "plt.imshow(np.transpose(grid_0[:100], (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "# Display the generated image grid with a larger size\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images_1\")\n",
    "plt.imshow(np.transpose(grid_1, (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "# Display the generated image grid with a larger size\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images_2\")\n",
    "plt.imshow(np.transpose(grid_2, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47a9f7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-04T13:00:29.790221Z",
     "iopub.status.idle": "2023-11-04T13:00:29.790601Z",
     "shell.execute_reply": "2023-11-04T13:00:29.790440Z",
     "shell.execute_reply.started": "2023-11-04T13:00:29.790422Z"
    }
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(1000, params['nz'], 1, 1).to(device)\n",
    "fixed_labels_0  = torch.full((1000,), 0).to(device)\n",
    "fixed_labels_1  = torch.full((1000,), 1).to(device)\n",
    "fixed_labels_2  = torch.full((1000,), 2).to(device)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    fake_0 = gen(fixed_noise, fixed_labels_0)\n",
    "    fake_1 = gen(fixed_noise, fixed_labels_1)\n",
    "    fake_2 = gen(fixed_noise, fixed_labels_2)\n",
    "\n",
    "\n",
    "\n",
    "real = []\n",
    "real_labels = []\n",
    "\n",
    "for batch_images, batch_labels in val_dataloader:\n",
    "    # Append the batch of images and labels to the collected lists\n",
    "    real.append(batch_images)\n",
    "    real_labels.append(batch_labels)\n",
    "\n",
    "    # Check if you have collected enough images\n",
    "    num_collected_images = sum(len(images) for images in real)\n",
    "    if num_collected_images >= 1000:\n",
    "        break\n",
    "\n",
    "# Concatenate the collected images and labels into tensors\n",
    "real = torch.cat(real)[:1000]\n",
    "real_images = torch.cat(real_labels)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two latent vectors for interpolation\n",
    "latent_vector1 = torch.randn(1, params['nz'], 1, 1).to(device)\n",
    "latent_vector2 = torch.randn(1, params['nz'], 1, 1).to(device)\n",
    "# Create a fixed label tensor with a dimension of 1\n",
    "fixed_labels_1 = torch.full((1,), 2).to(device)  # Change the label value as needed\n",
    "\n",
    "# Define the number of interpolation steps\n",
    "num_steps = 10  # You can adjust this number as needed\n",
    "\n",
    "# Perform linear interpolation between the two latent vectors\n",
    "# interpolated_latent_vectors = [latent_vector1 + (i / num_steps) * (latent_vector2 - latent_vector1) for i in range(num_steps)]\n",
    "\n",
    "# Interpolate between the latent representations\n",
    "interpolated_latents = []\n",
    "for alpha in torch.linspace(0, 1, num_steps):\n",
    "    interpolated_latent = alpha * latent_vector1 + (1 - alpha) * latent_vector2\n",
    "    interpolated_latents.append(interpolated_latent)\n",
    "\n",
    "\n",
    "# Generate images from the interpolated latent vectors\n",
    "generated_images = []\n",
    "for i, latent_vector in enumerate(interpolated_latents):\n",
    "    with torch.no_grad():\n",
    "        fake_image = gen(latent_vector, fixed_labels_1)\n",
    "        generated_images.append(fake_image)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_steps):\n",
    "    plt.subplot(2, num_steps // 2, i+1)\n",
    "    plt.imshow(generated_images[i][0].cpu().numpy().transpose(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Step {i+1}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Load a pre-trained Inception model\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "# Load the Inception-V3 model without using deprecated parameters\n",
    "inception_model = inception_v3(pretrained=True, num_classes=1000)\n",
    "inception_model.aux_logits = False  # Disable auxiliary logits\n",
    "inception_model.fc = nn.Identity()  # Remove the final classification layer\n",
    "\n",
    "def extract_inception_features(images, batch_size=32):\n",
    "    transform = transforms.Compose([transforms.Resize((299, 299))])\n",
    "    dataset = torch.utils.data.TensorDataset(images, torch.zeros(len(images)))  # Create a dummy target\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    inception_features = []\n",
    "\n",
    "    inception_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch_images, _ = batch\n",
    "            batch_images = transform(batch_images)  # Resize the images to 299x299\n",
    "            batch_features = inception_model(batch_images)\n",
    "            inception_features.append(batch_features)\n",
    "\n",
    "    inception_features = torch.cat(inception_features, 0)\n",
    "    return inception_features.cpu().numpy()\n",
    "\n",
    "\n",
    "# Function to calculate FID\n",
    "def calculate_fid(real_features, gen_features):\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    mu_gen = np.mean(gen_features, axis=0)\n",
    "    \n",
    "    cov_real = np.cov(real_features, rowvar=False)\n",
    "    cov_gen = np.cov(gen_features, rowvar=False)\n",
    "    \n",
    "    cov_sqrt = sqrtm(np.dot(cov_real, cov_gen))\n",
    "    \n",
    "    fid = np.sum((mu_real - mu_gen) ** 2) + np.trace(cov_real + cov_gen - 2 * cov_sqrt)\n",
    "    \n",
    "    return fid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97655f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca64f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(1000, params['nz'], 1, 1).to(device)\n",
    "fixed_labels_0  = torch.full((1000,), 0).to(device)\n",
    "fixed_labels_1  = torch.full((1000,), 1).to(device)\n",
    "fixed_labels_2  = torch.full((1000,), 2).to(device)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    fake_0 = gen(fixed_noise, fixed_labels_0)\n",
    "    fake_1 = gen(fixed_noise, fixed_labels_1)\n",
    "    fake_2 = gen(fixed_noise, fixed_labels_2)\n",
    "\n",
    "\n",
    "\n",
    "desired_labels = [0, 1, 2]\n",
    "num_collected_images = {label: 0 for label in desired_labels}\n",
    "collected_data = {label: {'images': [], 'labels': []} for label in desired_labels}\n",
    "\n",
    "for batch_images, batch_labels in train_dataloader:\n",
    "    for label in desired_labels:\n",
    "        # Find indices where labels are equal to the desired label\n",
    "        mask = (batch_labels == label).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(mask) > 0:\n",
    "            # Select only the samples with the desired label\n",
    "            selected_images = batch_images[mask]\n",
    "            selected_labels = batch_labels[mask]\n",
    "\n",
    "            # Append the batch of images and labels to the collected lists for the current label\n",
    "            collected_data[label]['images'].append(selected_images)\n",
    "            collected_data[label]['labels'].append(selected_labels)\n",
    "\n",
    "            # Update the number of collected images for the current label\n",
    "            num_collected_images[label] += len(selected_images)\n",
    "\n",
    "        # # Check if you have collected enough images for the current label (e.g., 1000 for each label)\n",
    "        # if num_collected_images[label] >= 1000:\n",
    "        #     break\n",
    "\n",
    "# Access collected data for label 0\n",
    "data_for_label_0 = collected_data[0]\n",
    "real_0 = data_for_label_0['images']\n",
    "real_label_0 = data_for_label_0['labels']\n",
    "\n",
    "# Access collected data for label 1\n",
    "data_for_label_1 = collected_data[1]\n",
    "real_1 = data_for_label_1['images']\n",
    "real_label_1 = data_for_label_1['labels']\n",
    "\n",
    "# Access collected data for label 2\n",
    "data_for_label_2 = collected_data[2]\n",
    "real_2 = data_for_label_2['images']\n",
    "real_label_2 = data_for_label_2['labels']\n",
    "\n",
    "# Now you have separate collected_data for labels 0, 1, and 2, each containing 1000 or fewer samples.\n",
    "\n",
    "# Concatenate the collected images and labels into tensors\n",
    "real_0 = torch.cat(real_0)[:1000]\n",
    "real_1 = torch.cat(real_1)[:1000]\n",
    "real_2 = torch.cat(real_2)[:1000]\n",
    "# real_images = torch.cat(real_labels)[:1000]\n",
    "\n",
    "# Assuming real_images and gen_images are PyTorch tensors or numpy arrays\n",
    "# Extract features from real and generated images\n",
    "\n",
    "inception_model = inception_model.to(device)\n",
    "real_0 = real_0.to(device)\n",
    "fake_0 = fake_0.to(device)\n",
    "\n",
    "real_features = extract_inception_features(real_0)\n",
    "gen_features = extract_inception_features(fake_0)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(real_features, gen_features)\n",
    "print(f\"FID 0: {np.real(fid)}\")\n",
    "\n",
    "inception_model = inception_model.to(device)\n",
    "real_1 = real_1.to(device)\n",
    "fake_1 = fake_1.to(device)\n",
    "\n",
    "real_2 = real_2.to(device)\n",
    "fake_2 = fake_2.to(device)\n",
    "\n",
    "real_features = extract_inception_features(real_1)\n",
    "gen_features = extract_inception_features(fake_1)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(real_features, gen_features)\n",
    "print(f\"FID 1 : {np.real(fid)}\")\n",
    "\n",
    "\n",
    "real_features = extract_inception_features(real_2)\n",
    "gen_features = extract_inception_features(fake_2)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(real_features, gen_features)\n",
    "print(f\"FID 2 : {np.real(fid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Model Training with LABEL as input\n",
    "- **Loss Curves:**\n",
    "  - Plotted generator and critic loss curves to monitor convergence and stability.\n",
    "\n",
    "- **Generated Images:**\n",
    "  - Displayed 10x10 grids of generated images for each class.\n",
    "\n",
    "### Evaluation\n",
    "- **FID Scores:**\n",
    "  - Calculated class-specific FID scores:\n",
    "    - Class 0: 140.39\n",
    "    - Class 1: 134.25\n",
    "    - Class 2: 171.90\n",
    "  - Assess the match between real and generated image distributions.\n",
    "\n",
    "- **Interpolation Results:**\n",
    "  - Generated images through linear interpolation for visual diversity.\n",
    "\n",
    "### Observations\n",
    "- **Training Stability:**\n",
    "  - Evaluated stability during training.\n",
    "\n",
    "- **Image Quality:**\n",
    "  - Assessed the quality of generated images.\n",
    "  - It was observed that FID of wild image is significantly higher than other two. \n",
    "  - Possible reason may be that object detection of wild images are much more complex than other two.\n",
    "\n",
    "  \n",
    "\n",
    "### Future Considerations\n",
    "- **Improvement Areas:**\n",
    "  - Identified potential areas for model enhancement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7338e",
   "metadata": {},
   "source": [
    "### CWGAN AS LABEL ONE HOT ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157b396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.247986Z",
     "iopub.status.busy": "2023-11-04T13:00:27.247052Z",
     "iopub.status.idle": "2023-11-04T13:00:27.254533Z",
     "shell.execute_reply": "2023-11-04T13:00:27.253427Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.247947Z"
    },
    "papermill": {
     "duration": 13.807954,
     "end_time": "2023-11-03T22:05:17.432768",
     "exception": false,
     "start_time": "2023-11-03T22:05:03.624814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transformations\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import PIL.Image as Image\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ignite.metrics.gan import FID\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48cd7f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.266351Z",
     "iopub.status.busy": "2023-11-04T13:00:27.266044Z",
     "iopub.status.idle": "2023-11-04T13:00:27.273430Z",
     "shell.execute_reply": "2023-11-04T13:00:27.272588Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.266324Z"
    },
    "papermill": {
     "duration": 0.016344,
     "end_time": "2023-11-03T22:05:17.455848",
     "exception": false,
     "start_time": "2023-11-03T22:05:17.439504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters to define the model.\n",
    "params = {\n",
    "    \"bsize\" : 64,# Batch size during training.\n",
    "    'imsize' : 128,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 3,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 64,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 64, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 200,# Number of training epochs.\n",
    "    'lr' : 1e-4,# Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'num_classes' : 3,# number of label in animal dataset\n",
    "    'gen_embs' : 100, # embedding of labels\n",
    "    'critic_iter' : 5, # number of critic iteration\n",
    "    'lambda_gp' : 10 # Gradient penality\n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d30122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.275441Z",
     "iopub.status.busy": "2023-11-04T13:00:27.275125Z",
     "iopub.status.idle": "2023-11-04T13:00:27.285315Z",
     "shell.execute_reply": "2023-11-04T13:00:27.284312Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.275415Z"
    },
    "papermill": {
     "duration": 0.079905,
     "end_time": "2023-11-03T22:05:17.542060",
     "exception": false,
     "start_time": "2023-11-03T22:05:17.462155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ffc46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.287234Z",
     "iopub.status.busy": "2023-11-04T13:00:27.286542Z",
     "iopub.status.idle": "2023-11-04T13:00:27.396858Z",
     "shell.execute_reply": "2023-11-04T13:00:27.396019Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.287201Z"
    },
    "papermill": {
     "duration": 17.084415,
     "end_time": "2023-11-03T22:05:34.632920",
     "exception": false,
     "start_time": "2023-11-03T22:05:17.548505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to your dataset and other hyperparameters\n",
    "data_dir_train = '/data4/home/anujkumar1/GAN/Datasets/images/train' \n",
    "data_dir_val = '/data4/home/anujkumar1/GAN/Datasets/images/val'\n",
    "shuffle_dataset = True  \n",
    "num_workers = 4  \n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((params['imsize'],params['imsize'])), \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root= data_dir_train, transform=transform)\n",
    "val_dataset = ImageFolder(root= data_dir_val , transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=params['bsize'],\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e33782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.398283Z",
     "iopub.status.busy": "2023-11-04T13:00:27.397943Z",
     "iopub.status.idle": "2023-11-04T13:00:27.409768Z",
     "shell.execute_reply": "2023-11-04T13:00:27.408803Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.398250Z"
    },
    "papermill": {
     "duration": 0.02058,
     "end_time": "2023-11-03T22:05:34.659742",
     "exception": false,
     "start_time": "2023-11-03T22:05:34.639162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = params['imsize']\n",
    "        self.num_classes = params['num_classes']\n",
    "        self.embed_size = params['gen_embs']\n",
    "        nz = params['nz']\n",
    "        ngf = params['ngf']\n",
    "        nc = params['nc']\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            nn.ConvTranspose2d(nz + self.num_classes, ngf * 32, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 32),\n",
    "            nn.ReLU(),\n",
    "             # img: 4x4\n",
    "            nn.ConvTranspose2d(ngf * 32, ngf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(),\n",
    "            # img: 8x8\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(),\n",
    "            # img: 16x16\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(),\n",
    "            # img: 32x32\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(),\n",
    "            # img: 64x64\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, kernel_size=4, stride=2, padding=1),\n",
    "            # Output: N x channels_img x 128 x 128\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        print('labes', labels.shape)\n",
    "        print('x:',x.shape)\n",
    "        labels = labels.unsqueeze(-1).unsqueeze(-1)\n",
    "        # embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        # print('embd:', embedding)\n",
    "        x = torch.cat([x, labels], 1)\n",
    "        print('x1:', x.shape)\n",
    "        print (self.main(x).shape)\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee572610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.412318Z",
     "iopub.status.busy": "2023-11-04T13:00:27.411974Z",
     "iopub.status.idle": "2023-11-04T13:00:27.425596Z",
     "shell.execute_reply": "2023-11-04T13:00:27.424718Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.412285Z"
    },
    "papermill": {
     "duration": 0.020673,
     "end_time": "2023-11-03T22:05:34.686449",
     "exception": false,
     "start_time": "2023-11-03T22:05:34.665776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = params['imsize']\n",
    "        nc = params['nc']\n",
    "        ndf = params['ndf']\n",
    "        num_classes = params['num_classes']\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc + num_classes, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Block 1\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "             # Block 4\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 16, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Output\n",
    "            nn.Conv2d(ndf * 16, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "        # Embedding for conditioning\n",
    "        self.embed = nn.Embedding(num_classes, self.img_size * self.img_size)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # print('x:', x.shape)\n",
    "        # print('lab:', labels.shape)\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 3, self.img_size, self.img_size)\n",
    "        # print('embs: ', embedding.shape)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        # print('x1:', x.shape)\n",
    "        # print('x2', self.main(x).shape)\n",
    "        return self.main(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef0fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# des1 = Encoder(params).to(device)\n",
    "des3 = Discriminator(params).to(device)\n",
    "des2 = Generator(params).to(device)\n",
    "for x,labels in train_dataloader:\n",
    "    x=x.to(device)\n",
    "    z = torch.randn(100,100,1,1).to(device)\n",
    "    labels = F.one_hot(labels, params['num_classes']).to(device)\n",
    "    fixed_labels_0 = torch.full((100,), 0)\n",
    "    fixed_labels_0 = F.one_hot(fixed_labels_0, params['num_classes']).to(device)\n",
    "    # print(des2(z,labels).shape)\n",
    "    print(des2(z,fixed_labels_0).shape)\n",
    "    # print(des3(x,labels).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af035e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.433909Z",
     "iopub.status.busy": "2023-11-04T13:00:27.433636Z",
     "iopub.status.idle": "2023-11-04T13:00:27.441909Z",
     "shell.execute_reply": "2023-11-04T13:00:27.441043Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.433885Z"
    },
    "papermill": {
     "duration": 0.015919,
     "end_time": "2023-11-03T22:05:34.708597",
     "exception": false,
     "start_time": "2023-11-03T22:05:34.692678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    \"\"\"\n",
    "    Initializes the weights of the layer, w.\n",
    "    \"\"\"\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('conv') != -1:\n",
    "        nn.init.normal_(w.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('bn') != -1:\n",
    "        nn.init.normal_(w.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(w.bias.data, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07c680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.444037Z",
     "iopub.status.busy": "2023-11-04T13:00:27.443738Z",
     "iopub.status.idle": "2023-11-04T13:00:27.975872Z",
     "shell.execute_reply": "2023-11-04T13:00:27.974943Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.444001Z"
    },
    "papermill": {
     "duration": 6.153807,
     "end_time": "2023-11-03T22:05:40.868718",
     "exception": false,
     "start_time": "2023-11-03T22:05:34.714911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the generator model with the provided parameters.\n",
    "gen = Generator(params).to(device)\n",
    "gen.apply(weights_init)\n",
    "critic = Discriminator(params).to(device)\n",
    "critic.apply(weights_init), gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097e657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.977174Z",
     "iopub.status.busy": "2023-11-04T13:00:27.976903Z",
     "iopub.status.idle": "2023-11-04T13:00:27.983271Z",
     "shell.execute_reply": "2023-11-04T13:00:27.982413Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.977150Z"
    },
    "papermill": {
     "duration": 0.015785,
     "end_time": "2023-11-03T22:05:40.891043",
     "exception": false,
     "start_time": "2023-11-03T22:05:40.875258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initializate optimizer\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5bb70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:27.984678Z",
     "iopub.status.busy": "2023-11-04T13:00:27.984436Z",
     "iopub.status.idle": "2023-11-04T13:00:27.994535Z",
     "shell.execute_reply": "2023-11-04T13:00:27.993820Z",
     "shell.execute_reply.started": "2023-11-04T13:00:27.984656Z"
    },
    "papermill": {
     "duration": 0.017003,
     "end_time": "2023-11-03T22:05:40.914799",
     "exception": false,
     "start_time": "2023-11-03T22:05:40.897796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(100, params['nz'], 1, 1).to(device)\n",
    "fixed_labels = torch.randint(0, 3, (100,))\n",
    "fixed_labels = F.one_hot(fixed_labels, params['num_classes']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59dd1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:28.006719Z",
     "iopub.status.busy": "2023-11-04T13:00:28.006454Z",
     "iopub.status.idle": "2023-11-04T13:00:28.342512Z",
     "shell.execute_reply": "2023-11-04T13:00:28.341657Z",
     "shell.execute_reply.started": "2023-11-04T13:00:28.006686Z"
    },
    "papermill": {
     "duration": 1.092895,
     "end_time": "2023-11-03T22:05:42.038519",
     "exception": false,
     "start_time": "2023-11-03T22:05:40.945624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### initialize FID wrapper\n",
    "fid_score = FID()\n",
    "#interpolate function to resize images to 299,299,3  which is the input size of inception network\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transformations.ToPILImage()(img)\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        arr.append(transformations.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937091ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the Generator model\n",
    "# gen.load_state_dict(torch.load('/data4/home/anujkumar1/GAN/generator_model.pth'))\n",
    "# # gen.eval()\n",
    "\n",
    "# # Load the Discriminator model\n",
    "# critic.load_state_dict(torch.load('/data4/home/anujkumar1/GAN/critic_model.pth'))\n",
    "# # critic.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ce310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:28.344102Z",
     "iopub.status.busy": "2023-11-04T13:00:28.343755Z",
     "iopub.status.idle": "2023-11-04T13:00:28.352117Z",
     "shell.execute_reply": "2023-11-04T13:00:28.351081Z",
     "shell.execute_reply.started": "2023-11-04T13:00:28.344074Z"
    },
    "papermill": {
     "duration": 0.019011,
     "end_time": "2023-11-03T22:05:42.065378",
     "exception": false,
     "start_time": "2023-11-03T22:05:42.046367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen.train()\n",
    "critic.train()\n",
    "#### gradient penalty function for WGAN-GP\n",
    "def gradient_penalty(critic,labels, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, labels)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069faada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T13:00:28.353859Z",
     "iopub.status.busy": "2023-11-04T13:00:28.353546Z",
     "iopub.status.idle": "2023-11-04T13:00:29.783227Z",
     "shell.execute_reply": "2023-11-04T13:00:29.781714Z",
     "shell.execute_reply.started": "2023-11-04T13:00:28.353833Z"
    },
    "papermill": {
     "duration": 26704.314486,
     "end_time": "2023-11-04T05:30:46.386881",
     "exception": false,
     "start_time": "2023-11-03T22:05:42.072395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "G_loss = []\n",
    "C_loss = []\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    #we will track the total loss of the generator and critic for each epoch over the entire dataset\n",
    "    #initialize the total loss of the generator and critic for each epoch to 0\n",
    "    total_loss_gen = 0\n",
    "    total_loss_critic = 0\n",
    "    \n",
    "    #move these to device\n",
    "\n",
    "    #have no gradient for these losse\n",
    "    # total_loss_gen = torch.no_grad()\n",
    "    # total_loss_critic = torch.no_grad()\n",
    "    \n",
    "    # Target labels \n",
    "    for batch_idx, (real, labels) in enumerate(tqdm(train_dataloader)):\n",
    "        #send labels to device\n",
    "        labels = F.one_hot(labels, params['num_classes'])\n",
    "        labels = labels.to(device)\n",
    "        batch_step = 0\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        \n",
    "        \n",
    "\n",
    "        # Train Critic: \n",
    "        for _ in range(params['critic_iter']):\n",
    "            noise = torch.randn(cur_batch_size, params['nz'], 1, 1).to(device)\n",
    "            if len(noise) != len(labels):\n",
    "                noise = noise[:len(labels)]\n",
    "            fake = gen(noise, labels)\n",
    "            critic_real = critic(real, labels).reshape(-1)\n",
    "            critic_fake = critic(fake, labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic, labels, real, fake, device=device)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + params['lambda_gp'] * gp\n",
    "            )\n",
    "            \n",
    "            \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "            \n",
    "        #trained critic\n",
    "        \n",
    "\n",
    "        # Train Generator: \n",
    "        gen_fake = critic(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #update the total loss of the generator and critic for each batch in the epoch\n",
    "        #add just the value no gradients\n",
    "        #have no gradient for these losse\n",
    "        #add just the value no gradientsfrom the loss_gen tensor\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            total_loss_gen += loss_gen.item()\n",
    "            total_loss_critic += loss_critic.item()\n",
    "        \n",
    "        \n",
    "        # # Print losses occasionally and print to tensorboard in a batch\n",
    "        # if (batch_idx + 1) % 23 == 0 and batch_idx > 0:\n",
    "        #     print(\n",
    "        #         f\"Epoch [{epoch+1}/{params['nepochs']}] Batch {batch_idx+1}/{len(train_dataloader)} \\\n",
    "        #           Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "        #     )\n",
    "            \n",
    "        batch_step += 1\n",
    "    \n",
    "        G_loss.append(loss_gen.item())\n",
    "        C_loss.append(loss_critic.item())   \n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{params['nepochs']}] Batch {batch_idx+1}/{len(train_dataloader)} \\\n",
    "                Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "    if (epoch+1)% 20 == 0:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake = gen(fixed_noise, fixed_labels)\n",
    "            # take out (up to) 32 examples\n",
    "            grid = vutils.make_grid(fake, nrow= 10, padding=2, normalize=True)[:100].cpu().numpy()\n",
    "            # Display the generated image grid with a larger size\n",
    "            plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Generated Images\")\n",
    "            plt.imshow(np.transpose(grid[:100], (1, 2, 0)))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        #AVERAGE LOSS---\n",
    "\n",
    "        #get average loss of generator and critic for each epoch\n",
    "        avg_loss_gen = total_loss_gen / len(train_dataloader)\n",
    "        avg_loss_critic = total_loss_critic / len(train_dataloader)\n",
    "\n",
    "    \n",
    "    step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training losses.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and critic Loss During Training\")\n",
    "plt.plot(G_loss,label=\"G\")\n",
    "plt.plot(C_loss,label=\"C\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97749760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the Generator model\n",
    "# torch.save(gen.state_dict(), 'generator_model.pth')\n",
    "\n",
    "# # Save the Discriminator model\n",
    "# torch.save(critic.state_dict(), 'critic_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Generator model\n",
    "torch.save(gen.state_dict(), 'generator_model_one_hot.pth')\n",
    "\n",
    "# Save the Discriminator model\n",
    "torch.save(critic.state_dict(), 'critic_model_one_hot.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe315ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Generator model\n",
    "# gen.load_state_dict(torch.load('/data4/home/anujkumar1/GAN/generator_model.pth'))\n",
    "gen.eval()\n",
    "\n",
    "# Load the Discriminator model\n",
    "# critic.load_state_dict(torch.load('/data4/home/anujkumar1/GAN/critic_model.pth'))\n",
    "critic.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(100, params['nz'], 1, 1).to(device)\n",
    "fixed_labels_0 = torch.full((100,), 0)\n",
    "fixed_labels_0 = F.one_hot(fixed_labels_0, params['num_classes']).to(device)\n",
    "print(gen(fixed_noise,fixed_labels_0).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf468d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "fixed_labels_2 = torch.full((100, 3), 2)\n",
    "print(fixed_labels_2.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e605a5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-04T13:00:29.786063Z",
     "iopub.status.idle": "2023-11-04T13:00:29.786530Z",
     "shell.execute_reply": "2023-11-04T13:00:29.786306Z",
     "shell.execute_reply.started": "2023-11-04T13:00:29.786285Z"
    }
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(100, params['nz'], 1, 1).to(device)\n",
    "fixed_labels_0 = torch.full((100,), 0)\n",
    "print(fixed_labels_0.shape)\n",
    "fixed_labels_0 = F.one_hot(fixed_labels_0, params['num_classes']).to(device)\n",
    "print(fixed_labels_0.shape)\n",
    "fixed_labels_1 = torch.full((100,), 1)\n",
    "fixed_labels_1 = F.one_hot(fixed_labels_1, params['num_classes']).to(device)\n",
    "fixed_labels_2 = torch.full((100,), 2)\n",
    "fixed_labels_2 = F.one_hot(fixed_labels_2, params['num_classes']).to(device)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    print(fixed_noise.shape)\n",
    "    print(fixed_labels_0.shape)\n",
    "    fake_0 = gen(fixed_noise, fixed_labels_0)\n",
    "    fake_1 = gen(fixed_noise, fixed_labels_1)\n",
    "    fake_2 = gen(fixed_noise, fixed_labels_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280134e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-04T13:00:29.787537Z",
     "iopub.status.idle": "2023-11-04T13:00:29.787963Z",
     "shell.execute_reply": "2023-11-04T13:00:29.787764Z",
     "shell.execute_reply.started": "2023-11-04T13:00:29.787743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape the generated images into a 10x10 grid\n",
    "grid_0 = vutils.make_grid(fake_0, nrow= 10, padding=2, normalize=True)[:100].cpu().numpy()\n",
    "grid_1 = vutils.make_grid(fake_1, nrow= 10, padding=2, normalize=True)[:100].cpu().numpy()\n",
    "grid_2 = vutils.make_grid(fake_2, nrow= 10, padding=2, normalize=True)[:100].cpu().numpy()\n",
    "\n",
    "# Display the generated image grid with a larger size\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images_0\")\n",
    "plt.imshow(np.transpose(grid_0[:100], (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "# Display the generated image grid with a larger size\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images_1\")\n",
    "plt.imshow(np.transpose(grid_1, (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "# Display the generated image grid with a larger size\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images_2\")\n",
    "plt.imshow(np.transpose(grid_2, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a19af",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-04T13:00:29.790221Z",
     "iopub.status.idle": "2023-11-04T13:00:29.790601Z",
     "shell.execute_reply": "2023-11-04T13:00:29.790440Z",
     "shell.execute_reply.started": "2023-11-04T13:00:29.790422Z"
    }
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(1000, params['nz'], 1, 1).to(device)\n",
    "fixed_labels_0 = torch.full((1000,), 0)\n",
    "fixed_labels_0 = F.one_hot(fixed_labels_0, params['num_classes']).to(device)\n",
    "fixed_labels_1 = torch.full((1000,), 1)\n",
    "fixed_labels_1 = F.one_hot(fixed_labels_1, params['num_classes']).to(device)\n",
    "fixed_labels_2 = torch.full((1000,), 2)\n",
    "fixed_labels_2 = F.one_hot(fixed_labels_2, params['num_classes']).to(device)\n",
    "\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    fake_0 = gen(fixed_noise, fixed_labels_0)\n",
    "    fake_1 = gen(fixed_noise, fixed_labels_1)\n",
    "    fake_2 = gen(fixed_noise, fixed_labels_2)\n",
    "\n",
    "\n",
    "\n",
    "real = []\n",
    "real_labels = []\n",
    "\n",
    "for batch_images, batch_labels in val_dataloader:\n",
    "    # Append the batch of images and labels to the collected lists\n",
    "    real.append(batch_images)\n",
    "    real_labels.append(batch_labels)\n",
    "\n",
    "    # Check if you have collected enough images\n",
    "    num_collected_images = sum(len(images) for images in real)\n",
    "    if num_collected_images >= 1000:\n",
    "        break\n",
    "\n",
    "# Concatenate the collected images and labels into tensors\n",
    "real = torch.cat(real)[:1000]\n",
    "real_images = torch.cat(real_labels)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two latent vectors for interpolation\n",
    "latent_vector1 = torch.randn(1, params['nz'], 1, 1).to(device)\n",
    "latent_vector2 = torch.randn(1, params['nz'], 1, 1).to(device)\n",
    "# Create a fixed label tensor with a dimension of 1\n",
    "fixed_labels_1 = torch.full((1,), 1).to(device)  # Change the label value as needed\n",
    "fixed_labels_1 = F.one_hot(fixed_labels_1, params['num_classes']).to(device)\n",
    "# Define the number of interpolation steps\n",
    "num_steps = 10  # You can adjust this number as needed\n",
    "\n",
    "# Perform linear interpolation between the two latent vectors\n",
    "# interpolated_latent_vectors = [latent_vector1 + (i / num_steps) * (latent_vector2 - latent_vector1) for i in range(num_steps)]\n",
    "\n",
    "# Interpolate between the latent representations\n",
    "interpolated_latents = []\n",
    "for alpha in torch.linspace(0, 1, num_steps):\n",
    "    interpolated_latent = alpha * latent_vector1 + (1 - alpha) * latent_vector2\n",
    "    interpolated_latents.append(interpolated_latent)\n",
    "\n",
    "\n",
    "# Generate images from the interpolated latent vectors\n",
    "generated_images = []\n",
    "for i, latent_vector in enumerate(interpolated_latents):\n",
    "    with torch.no_grad():\n",
    "        fake_image = gen(latent_vector, fixed_labels_1)\n",
    "        generated_images.append(fake_image)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_steps):\n",
    "    plt.subplot(2, num_steps // 2, i+1)\n",
    "    plt.imshow(generated_images[i][0].cpu().numpy().transpose(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Step {i+1}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Load a pre-trained Inception model\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "# Load the Inception-V3 model without using deprecated parameters\n",
    "inception_model = inception_v3(pretrained=True, num_classes=1000)\n",
    "inception_model.aux_logits = False  # Disable auxiliary logits\n",
    "inception_model.fc = nn.Identity()  # Remove the final classification layer\n",
    "\n",
    "def extract_inception_features(images, batch_size=32):\n",
    "    transform = transforms.Compose([transforms.Resize((299, 299))])\n",
    "    dataset = torch.utils.data.TensorDataset(images, torch.zeros(len(images)))  # Create a dummy target\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    inception_features = []\n",
    "\n",
    "    inception_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch_images, _ = batch\n",
    "            batch_images = transform(batch_images)  # Resize the images to 299x299\n",
    "            batch_features = inception_model(batch_images)\n",
    "            inception_features.append(batch_features)\n",
    "\n",
    "    inception_features = torch.cat(inception_features, 0)\n",
    "    return inception_features.cpu().numpy()\n",
    "\n",
    "\n",
    "# Function to calculate FID\n",
    "def calculate_fid(real_features, gen_features):\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    mu_gen = np.mean(gen_features, axis=0)\n",
    "    \n",
    "    cov_real = np.cov(real_features, rowvar=False)\n",
    "    cov_gen = np.cov(gen_features, rowvar=False)\n",
    "    \n",
    "    cov_sqrt = sqrtm(np.dot(cov_real, cov_gen))\n",
    "    \n",
    "    fid = np.sum((mu_real - mu_gen) ** 2) + np.trace(cov_real + cov_gen - 2 * cov_sqrt)\n",
    "    \n",
    "    return fid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d350db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(1000, params['nz'], 1, 1).to(device)\n",
    "fixed_labels_0 = torch.full((1000,), 0)\n",
    "fixed_labels_0 = F.one_hot(fixed_labels_0, params['num_classes']).to(device)\n",
    "fixed_labels_1 = torch.full((1000,), 1)\n",
    "fixed_labels_1 = F.one_hot(fixed_labels_1, params['num_classes']).to(device)\n",
    "fixed_labels_2 = torch.full((1000,), 2)\n",
    "fixed_labels_2 = F.one_hot(fixed_labels_2, params['num_classes']).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_0 = gen(fixed_noise, fixed_labels_0)\n",
    "    fake_1 = gen(fixed_noise, fixed_labels_1)\n",
    "    fake_2 = gen(fixed_noise, fixed_labels_2)\n",
    "\n",
    "\n",
    "\n",
    "desired_labels = [0, 1, 2]\n",
    "num_collected_images = {label: 0 for label in desired_labels}\n",
    "collected_data = {label: {'images': [], 'labels': []} for label in desired_labels}\n",
    "\n",
    "for batch_images, batch_labels in train_dataloader:\n",
    "    for label in desired_labels:\n",
    "        # Find indices where labels are equal to the desired label\n",
    "        mask = (batch_labels == label).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(mask) > 0:\n",
    "            # Select only the samples with the desired label\n",
    "            selected_images = batch_images[mask]\n",
    "            selected_labels = batch_labels[mask]\n",
    "\n",
    "            # Append the batch of images and labels to the collected lists for the current label\n",
    "            collected_data[label]['images'].append(selected_images)\n",
    "            collected_data[label]['labels'].append(selected_labels)\n",
    "\n",
    "            # Update the number of collected images for the current label\n",
    "            num_collected_images[label] += len(selected_images)\n",
    "\n",
    "        # # Check if you have collected enough images for the current label (e.g., 1000 for each label)\n",
    "        # if num_collected_images[label] >= 1000:\n",
    "        #     break\n",
    "\n",
    "# Access collected data for label 0\n",
    "data_for_label_0 = collected_data[0]\n",
    "real_0 = data_for_label_0['images']\n",
    "real_label_0 = data_for_label_0['labels']\n",
    "\n",
    "# Access collected data for label 1\n",
    "data_for_label_1 = collected_data[1]\n",
    "real_1 = data_for_label_1['images']\n",
    "real_label_1 = data_for_label_1['labels']\n",
    "\n",
    "# Access collected data for label 2\n",
    "data_for_label_2 = collected_data[2]\n",
    "real_2 = data_for_label_2['images']\n",
    "real_label_2 = data_for_label_2['labels']\n",
    "\n",
    "# Now you have separate collected_data for labels 0, 1, and 2, each containing 1000 or fewer samples.\n",
    "\n",
    "# Concatenate the collected images and labels into tensors\n",
    "real_0 = torch.cat(real_0)[:1000]\n",
    "real_1 = torch.cat(real_1)[:1000]\n",
    "real_2 = torch.cat(real_2)[:1000]\n",
    "# real_images = torch.cat(real_labels)[:1000]\n",
    "\n",
    "# Assuming real_images and gen_images are PyTorch tensors or numpy arrays\n",
    "# Extract features from real and generated images\n",
    "\n",
    "inception_model = inception_model.to(device)\n",
    "real_0 = real_0.to(device)\n",
    "fake_0 = fake_0.to(device)\n",
    "\n",
    "real_features = extract_inception_features(real_0)\n",
    "gen_features = extract_inception_features(fake_0)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(real_features, gen_features)\n",
    "print(f\"FID 0: {np.real(fid)}\")\n",
    "\n",
    "inception_model = inception_model.to(device)\n",
    "real_1 = real_1.to(device)\n",
    "fake_1 = fake_1.to(device)\n",
    "\n",
    "real_2 = real_2.to(device)\n",
    "fake_2 = fake_2.to(device)\n",
    "\n",
    "real_features = extract_inception_features(real_1)\n",
    "gen_features = extract_inception_features(fake_1)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(real_features, gen_features)\n",
    "print(f\"FID 1 : {np.real(fid)}\")\n",
    "\n",
    "\n",
    "real_features = extract_inception_features(real_2)\n",
    "gen_features = extract_inception_features(fake_2)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(real_features, gen_features)\n",
    "print(f\"FID 2 : {np.real(fid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with LABEL ONE-HOT ENCODER as input\n",
    "- **Loss Curves:**\n",
    "  - Plotted generator and critic loss curves to track convergence.\n",
    "\n",
    "- **Generated Images:**\n",
    "  - Showcased 10x10 grids of generated images for each class.\n",
    "\n",
    "### Evaluation\n",
    "- **FID Scores:**\n",
    "  - Calculated class-specific FID scores:\n",
    "    - Class 0: 148.39\n",
    "    - Class 1: 144.09\n",
    "    - Class 2: 175.76\n",
    "  - Assessed agreement between real and generated distributions.\n",
    "\n",
    "- **Interpolation Results:**\n",
    "  - Generated images through linear interpolation for visual exploration.\n",
    "\n",
    "### Observations\n",
    "- **Training Stability:**\n",
    "  - Examined stability and robustness during training.\n",
    "\n",
    "- **Image Quality:**\n",
    "  - Evaluated the quality of generated images.\n",
    "  - It was observed that FID of wild image is significantly higher than other two. \n",
    "  - Possible reason may be that object detection of wild images are much more complex than other two.\n",
    "\n",
    "- **Obseravtions between two label inputs**\n",
    "  - There is no significant change in FID score whether label input is one hot encoder or label as embedding\n",
    "  \n",
    "\n",
    "### Future Considerations\n",
    "- **Enhancements:**\n",
    "  - Identified potential areas for model improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e81ee",
   "metadata": {},
   "source": [
    "### BIGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transformations\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init, Module, ReLU, Sequential, ModuleList, Conv2d, MaxPool2d, LeakyReLU, Flatten, Linear, Sigmoid, ConvTranspose2d, BatchNorm2d, Tanh, Dropout2d, Dropout\n",
    "from torch.nn.functional import binary_cross_entropy as bce\n",
    "import PIL.Image as Image\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ignite.metrics.gan import FID\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f324161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to define the model.\n",
    "params = {\n",
    "    \"bsize\" : 128,# Batch size during training.\n",
    "    'imsize' : 128,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 3,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 64,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 64, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 200,# Number of training epochs.\n",
    "    'lr' : 1e-4,# Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'num_classes' : 3,# number of label in animal dataset\n",
    "    'gen_embs' : 100, # embedding of labels\n",
    "    'critic_iter' : 5, # number of critic iteration\n",
    "    'lambda_gp' : 10 # Gradient penality\n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset and other hyperparameters\n",
    "data_dir_train = '/data4/home/anujkumar1/GAN/Datasets/images/train' \n",
    "data_dir_val = '/data4/home/anujkumar1/GAN/Datasets/images/val'\n",
    "shuffle_dataset = True  \n",
    "num_workers = 4  \n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((params['imsize'],params['imsize'])), \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root= data_dir_train, transform=transform)\n",
    "val_dataset = ImageFolder(root= data_dir_val , transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=params['bsize'],\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0eed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aed970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super(Encoder, self).__init__()\n",
    "        ndf = params['ndf']\n",
    "        nc = params['nc']\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(ndf * 16, ndf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(ndf * 16, 100, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd501aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super(Generator, self).__init__()\n",
    "        ngf = params['ngf']\n",
    "        nz = params['nz']\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super(Discriminator, self).__init__()\n",
    "        ndf = params['ndf']\n",
    "        nc = params['nc']\n",
    "        nz = params['nz']\n",
    "\n",
    "        self.conv_blocks_x = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, ndf * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, ndf * 8, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv_blocks_z = nn.Sequential(\n",
    "            nn.Conv2d(nz, ndf * 8, 1, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, ndf * 8, 1, 1, 0),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv_blocks_combined = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 16, ndf * 32, 1, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(ndf * 32, ndf * 32, 1, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(ndf * 32, 1, 1, 1, 0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x = self.conv_blocks_x(x)\n",
    "        z = self.conv_blocks_z(z)\n",
    "        xz = torch.cat((x, z), dim=1)\n",
    "        xz = self.conv_blocks_combined(xz)\n",
    "        return xz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "des1 = Encoder(params).to(device)\n",
    "des3 = Discriminator(params).to(device)\n",
    "des2 = Generator(params).to(device)\n",
    "for x,_ in train_dataloader:\n",
    "    x=x.to(device)\n",
    "    z = torch.randn(params['bsize'],100,1,1).to(device)\n",
    "    print(des1(x).shape)\n",
    "    print(des2(z).shape)\n",
    "    print(des3(x,z).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = params['nepochs']\n",
    "\n",
    "latent_dim = params['nz']\n",
    "\n",
    "# discriminator = Discriminator().to(device)\n",
    "# generator =  Generator().to(device)\n",
    "# encoder = Encoder().to(device)\n",
    "\n",
    "# discriminator.apply(weights_init)\n",
    "# encoder.apply(weights_init)\n",
    "# generator.apply(weights_init)\n",
    "\n",
    "\n",
    "model = nn.Sequential(Discriminator(params), Generator(params), Encoder(params)).to(device)\n",
    "model.apply(weights_init)\n",
    "\n",
    "\n",
    "\n",
    "optim_generator = torch.optim.Adam([{'params' : model[1].parameters()},\n",
    "                        {'params' : model[2].parameters()}],lr=0.0002,betas=(0.5,0.999))\n",
    "optim_discriminator = torch.optim.Adam(model[0].parameters(),lr=0.0002,betas=(0.5,0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(k,Z=params['nz']):\n",
    "    return torch.randn((k, Z, 1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc730a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    i = 0\n",
    "    d_loss = 0.0\n",
    "    ge_loss  = 0.0\n",
    "\n",
    "    # num_training_steps = len(train_dataloader)\n",
    "    # progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for batch_idx, (X,_) in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "\n",
    "        x_real = X.to(device)\n",
    "        z_real = sample(params['bsize'])\n",
    "\n",
    "        z_fake, x_fake = model[2](x_real), model[1](z_real)\n",
    "        real_preds, fake_preds = model[0](x_real, z_fake), model[0](x_fake, z_real)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy(real_preds.squeeze(), torch.ones((len(real_preds),)).to(device)) + F.binary_cross_entropy(fake_preds.squeeze(), torch.zeros((len(fake_preds),)).to(device))\n",
    "        gen_loss = F.binary_cross_entropy(real_preds.squeeze(), torch.zeros((len(real_preds),)).to(device)) + F.binary_cross_entropy(fake_preds.squeeze(), torch.ones((len(fake_preds),)).to(device))\n",
    "\n",
    "        d_loss += disc_loss.item()\n",
    "        ge_loss += gen_loss.item()\n",
    "\n",
    "        \n",
    "        if not i%5 == 0: # train discriminator 5 times & generator once\n",
    "            optim_discriminator.zero_grad()\n",
    "            disc_loss.backward()\n",
    "            optim_discriminator.step()\n",
    "        else:\n",
    "            optim_generator.zero_grad()\n",
    "            gen_loss.backward()\n",
    "            optim_generator.step()\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "        # progress_bar.update(1)\n",
    "    \n",
    "\n",
    "    print(\"[Epoch %d/%d] [D loss: %f] [GE loss: %f] \"\n",
    "            % (epoch+1, num_epochs, d_loss, ge_loss))\n",
    "\n",
    "    \"\"\"\n",
    "    if (epoch+1)%10 == 0:\n",
    "        torch.save(G.state_dict() , f\"./models1/{epoch+1}_G.pth\")\n",
    "        torch.save(E.state_dict() , f\"./models1/{epoch+1}_D.pth\")\n",
    "        torch.save(D.state_dict() , f\"./models1/{epoch+1}_E.pth\")\n",
    "    \"\"\"\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        z = torch.randn(100, 100, 1, 1).to(device)\n",
    "        z = model[1](z).cpu()\n",
    "        # save_image(z,f\"/data4/home/anujkumar1/Image/{epoch + 1}_Generated.png\", nrow=10)\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Generated Images\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(z[:100], padding=5, nrow=10,normalize=True).cpu(),(1,2,0)))\n",
    "        # plt.savefig(f\"./generated1/{epoch+1}_Generated.png\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'Bigan_model_copy1.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f022962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Generator model\n",
    "device = torch.device(\"cuda:3\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device, \" will be used.\\n\")\n",
    "model = nn.Sequential(Discriminator(params), Generator(params), Encoder(params)).to('cpu')\n",
    "model.apply(weights_init)\n",
    "optim_generator = torch.optim.Adam([{'params' : model[1].parameters()},\n",
    "                        {'params' : model[2].parameters()}],lr=0.0002,betas=(0.5,0.999))\n",
    "optim_discriminator = torch.optim.Adam(model[0].parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "\n",
    "model.load_state_dict(torch.load('Bigan_model_copy1.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf74ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(100, 100, 1, 1).cpu()\n",
    "with torch.no_grad():\n",
    "    generated_imgs = model[1](z).detach().cpu()\n",
    "\n",
    "# Reshape the generated images into a 10x10 grid\n",
    "grid = vutils.make_grid(generated_imgs, nrow= 10, padding=2, normalize=True)\n",
    "\n",
    "# Display the generated image grid with a larger size\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figsize as needed\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train= []\n",
    "\n",
    "for batch_images, batch_labels in train_dataloader:\n",
    "    # Append the batch of images and labels to the collected lists\n",
    "    batch_images = batch_images#.to(device)\n",
    "    z = model[2](batch_images)\n",
    "    z=z.cpu()\n",
    "    X_train.append(z)\n",
    "    y_train.append(batch_labels)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for batch_images, batch_labels in val_dataloader:\n",
    "    # Append the batch of images and labels to the collected lists\n",
    "    batch_images = batch_images#.to(device)\n",
    "    z = model[2](batch_images)\n",
    "    z=z.cpu()\n",
    "    X_val.append(z)\n",
    "    y_val.append(batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3521f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.cat(X_train).cpu().detach().numpy()\n",
    "X_val = torch.cat(X_val).cpu().detach().numpy()\n",
    "y_train = torch.cat(y_train).cpu().detach().numpy()\n",
    "y_val = torch.cat(y_val).cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a222c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e99cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Reshape the X_train data to be 2D\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "print(X_train.shape,X_val.shape)\n",
    "# Create an SVM classifier\n",
    "clf = SVC(kernel='rbf')\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(multi_class='multinomial',solver='lbfgs')\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Load a pre-trained Inception model\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "# Load the Inception-V3 model without using deprecated parameters\n",
    "inception_model = inception_v3(pretrained=True, num_classes=1000)\n",
    "inception_model.aux_logits = False  # Disable auxiliary logits\n",
    "inception_model.fc = nn.Identity()  # Remove the final classification layer\n",
    "\n",
    "def extract_inception_features(images, batch_size=32):\n",
    "    transform = transforms.Compose([transforms.Resize((299, 299))])\n",
    "    dataset = torch.utils.data.TensorDataset(images, torch.zeros(len(images)))  # Create a dummy target\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    inception_features = []\n",
    "\n",
    "    inception_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch_images, _ = batch\n",
    "            batch_images = transform(batch_images)  # Resize the images to 299x299\n",
    "            batch_features = inception_model(batch_images)\n",
    "            inception_features.append(batch_features)\n",
    "\n",
    "    inception_features = torch.cat(inception_features, 0)\n",
    "    return inception_features.numpy()\n",
    "\n",
    "\n",
    "# Function to calculate FID\n",
    "def calculate_fid(real_features, gen_features):\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    mu_gen = np.mean(gen_features, axis=0)\n",
    "    \n",
    "    cov_real = np.cov(real_features, rowvar=False)\n",
    "    cov_gen = np.cov(gen_features, rowvar=False)\n",
    "    \n",
    "    cov_sqrt = sqrtm(np.dot(cov_real, cov_gen))\n",
    "    \n",
    "    fid = np.sum((mu_real - mu_gen) ** 2) + np.trace(cov_real + cov_gen - 2 * cov_sqrt)\n",
    "    \n",
    "    return fid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416097d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(1000, params['nz'], 1, 1, device='cpu')\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    generated_imgs = model[1](noise).detach().cpu()\n",
    "    \n",
    "real = []\n",
    "real_labels = []\n",
    "\n",
    "for batch_images, batch_labels in val_dataloader:\n",
    "    # Append the batch of images and labels to the collected lists\n",
    "    real.append(batch_images)\n",
    "    real_labels.append(batch_labels)\n",
    "\n",
    "    # Check if you have collected enough images\n",
    "    num_collected_images = sum(len(images) for images in real)\n",
    "    if num_collected_images >= 1000:\n",
    "        break\n",
    "\n",
    "\n",
    "real_imgs = torch.cat(real)[:1000]\n",
    "real_labels = torch.cat(real_labels)[:1000]\n",
    "real_imgs = real_imgs.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming real_images and gen_images are PyTorch tensors or numpy arrays\n",
    "# Extract features from real and generated images\n",
    "real_features = extract_inception_features(real_imgs)\n",
    "gen_features = extract_inception_features(generated_imgs)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(real_features, gen_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FID: {np.real(fid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Bi-GAN Implementation:**\n",
    "   - Developed a Bi-GAN model for image generation using the provided dataset, incorporating encoder, generator, and discriminator components.\n",
    "\n",
    "2. **Image Generation and Visualization:**\n",
    "   - Trained the model to generate images and plotted a 10x10 grid to showcase the diversity and quality of the generated samples.\n",
    "\n",
    "3. **Posterior Inference and Classifier Training:**\n",
    "   - Leveraged the trained generator for posterior inference on real data, extracting latent vectors via the encoder.\n",
    "   - Trained linear and SVM classifiers using the inferred latent vectors to classify among the three classes.\n",
    "\n",
    "4. **Classifier Evaluation and FID Score:**\n",
    "   - Assessed the performance of classifiers on a validation set, considering accuracy, precision, recall, and F1-score.\n",
    "   - Obtained Accuracy of 90.86% using Loistic Regression\n",
    "   - Obtained Accuracy of 92.86% using SVM - RBF kernel\n",
    "   - Utilized the FID score (99.24) to quantify the similarity between generated and real data distributions.\n",
    "\n",
    "5. **Conclusion and Future Work:**\n",
    "   - Concluded with insights into the Bi-GAN's image generation capabilities and the effectiveness of inferred latent vectors for downstream classification.\n",
    "   - Outlined potential directions for future improvements or research based on the obtained results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81275e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9051.279194,
   "end_time": "2023-10-31T19:23:52.767997",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-31T16:53:01.488803",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
