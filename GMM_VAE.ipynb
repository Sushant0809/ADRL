{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.distributions as dist\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208528c0",
   "metadata": {},
   "source": [
    "# Question 1: GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, num_components, num_iterations, tolerance, device='cpu'):\n",
    "        self.num_components = num_components\n",
    "        self.num_iterations = num_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.device = device\n",
    "        self.means = None\n",
    "        self.covariances = None\n",
    "        self.mixing_coefficients = None\n",
    "        self.likelihoods = []\n",
    "\n",
    "    def initialize_parameters(self, data):\n",
    "        num_samples, num_features = data.shape\n",
    "        self.means = data[torch.randint(0, num_samples, (self.num_components,))].to(self.device)\n",
    "        self.covariances = torch.stack([torch.eye(num_features) for _ in range(self.num_components)]).to(self.device)\n",
    "        self.mixing_coefficients = (torch.ones(self.num_components) / self.num_components).to(self.device)\n",
    "\n",
    "    def E_Step(self, data):\n",
    "        responsibilities = torch.zeros(data.shape[0], self.num_components).to(self.device)\n",
    "        for k in range(self.num_components):\n",
    "            mvn = dist.MultivariateNormal(self.means[k], self.covariances[k])\n",
    "            responsibilities[:, k] = self.mixing_coefficients[k] * mvn.log_prob(data)\n",
    "        return F.softmax(responsibilities, dim=1)\n",
    "\n",
    "    def M_Step(self, data, responsibilities):\n",
    "        self.mixing_coefficients = responsibilities.mean(dim=0)\n",
    "        weighted_data = data.unsqueeze(1) * responsibilities.unsqueeze(2)\n",
    "        self.means = weighted_data.sum(dim=0) / responsibilities.sum(dim=0).unsqueeze(1)\n",
    "        for k in range(self.num_components):\n",
    "            centered_data = data - self.means[k]\n",
    "            self.covariances[k] = torch.matmul(centered_data.t(), centered_data * responsibilities[:, k].unsqueeze(1)) / responsibilities[:, k].sum()\n",
    "            self.covariances[k] += 1e-6 * torch.eye(data.shape[1])\n",
    "\n",
    "    def compute_likelihood(self, data):\n",
    "        likelihoods = torch.zeros(data.shape[0], self.num_components).to(self.device)\n",
    "        for k in range(self.num_components):\n",
    "            mvn = dist.MultivariateNormal(self.means[k], self.covariances[k])\n",
    "            likelihoods[:, k] = self.mixing_coefficients[k] * mvn.log_prob(data)\n",
    "        return likelihoods.sum(dim=1).mean()\n",
    "\n",
    "    def train(self, data):\n",
    "        self.initialize_parameters(data)\n",
    "        for iteration in range(self.num_iterations):\n",
    "            responsibilities = self.E_Step(data)\n",
    "            self.M_Step(data, responsibilities)\n",
    "            reg_term = 1e-6\n",
    "            for k in range(self.num_components):\n",
    "                self.covariances[k].add_(torch.eye(self.covariances[k].size(0)) * reg_term)\n",
    "            log_likelihood = self.compute_likelihood(data)\n",
    "            self.likelihoods.append(log_likelihood.item())\n",
    "            if iteration > 0 and abs(self.likelihoods[-1] - self.likelihoods[-2]) < self.tolerance:\n",
    "                break\n",
    "\n",
    "    def generate_samples(self, num_samples=100):\n",
    "        samples = torch.zeros(num_samples, self.means.shape[1]).to(self.device)\n",
    "        for i in range(num_samples):\n",
    "            component = np.random.choice(self.num_components, p=self.mixing_coefficients.detach().cpu().numpy())\n",
    "            mvn = dist.MultivariateNormal(self.means[component], self.covariances[component])\n",
    "            samples[i] = mvn.sample()\n",
    "        return samples\n",
    "\n",
    "    def plot_likelihood_curve(self):\n",
    "        plt.plot(range(len(self.likelihoods)), self.likelihoods)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Log Likelihood\")\n",
    "        plt.title(\"Likelihood Curve\")\n",
    "        plt.show()\n",
    "        \n",
    "    def generate_samples(self, num_samples=100):\n",
    "        num_components, num_features = self.means.shape\n",
    "        generated_samples = torch.zeros(num_samples, num_features)\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            component = torch.randint(0, num_components, (1,))\n",
    "            mvn = dist.MultivariateNormal(self.means[component], self.covariances[component])\n",
    "            sample = mvn.sample()\n",
    "            generated_samples[_] = sample\n",
    "        \n",
    "        return generated_samples\n",
    "\n",
    "    def visualize_samples(self, num_samples=100):\n",
    "        generated_samples = self.generate_samples(num_samples)\n",
    "        \n",
    "        grid_size = (10, 10)\n",
    "        fig, axarr = plt.subplots(*grid_size, figsize=(10, 10))\n",
    "        \n",
    "        for i in range(grid_size[0]):\n",
    "            for j in range(grid_size[1]):\n",
    "                sample = generated_samples[i * grid_size[1] + j].view(3, 16, 16).permute(1,2,0).numpy()\n",
    "                sample = np.clip(sample, 0, 1)  # Clip the values of the sample to be in the range [0,1]\n",
    "                axarr[i, j].imshow(sample)\n",
    "                axarr[i, j].axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Generated Samples ({self.num_components} Components)\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def calculate_entropy(self, labels):\n",
    "        # Calculate entropy of labels using PyTorch\n",
    "        unique_labels, label_counts = torch.unique(labels, return_counts=True)\n",
    "        probabilities = label_counts.float() / len(labels)\n",
    "        entropy_value = -torch.sum(probabilities * torch.log2(probabilities))\n",
    "        return entropy_value\n",
    "\n",
    "    def calculate_mutual_information(self, class_labels, cluster_assignments):\n",
    "        # Convert class_labels and cluster_assignments to 'Float'\n",
    "        class_labels = class_labels.float()\n",
    "        cluster_assignments = cluster_assignments.float()\n",
    "\n",
    "        # Calculate mutual information between class labels and cluster assignments using PyTorch\n",
    "        max_bins = int((class_labels.max() + 1) * (cluster_assignments.max() + 1))\n",
    "\n",
    "        joint_distribution = torch.histc(\n",
    "            class_labels * (cluster_assignments.max() + 1) + cluster_assignments,\n",
    "            bins=max_bins,\n",
    "            min=0,\n",
    "            max=max_bins\n",
    "        )\n",
    "        joint_distribution = joint_distribution.view(int(class_labels.max() + 1), int(cluster_assignments.max() + 1))\n",
    "        p_class = joint_distribution.sum(dim=1) / len(class_labels)\n",
    "        p_cluster = joint_distribution.sum(dim=0) / len(class_labels)\n",
    "        p_joint = joint_distribution / len(class_labels)\n",
    "\n",
    "        mutual_info = torch.sum(p_joint * torch.log2(p_joint / (torch.outer(p_class, p_cluster) + 1e-12) + 1e-12))\n",
    "        return mutual_info\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_nmi(self, class_labels, cluster_assignments):\n",
    "        # Calculate mutual information between class labels and cluster assignments\n",
    "        mutual_info = self.calculate_mutual_information(class_labels, cluster_assignments)\n",
    "\n",
    "        # Calculate entropy of class labels and cluster assignments\n",
    "        entropy_class = self.calculate_entropy(class_labels)\n",
    "        entropy_cluster = self.calculate_entropy(cluster_assignments)\n",
    "\n",
    "        # Calculate NMI using the formula\n",
    "        nmi = 2 * mutual_info / (entropy_class + entropy_cluster)\n",
    "        return nmi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d12ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the path to your dataset and other hyperparameters\n",
    "data_dir_train = 'afhq/train/' \n",
    "data_dir_val = 'afhq/val/'\n",
    "shuffle_dataset = True  \n",
    "num_workers = 4  \n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16,16)), \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root='afhq/train/', transform=transform)\n",
    "#val_dataset = ImageFolder(root='afhq/val/', transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "data = []\n",
    "for images, _ in train_dataloader:\n",
    "    flattened_images = images.view(images.size(0), -1)  # Flatten each image\n",
    "    data.append(flattened_images)\n",
    "\n",
    "data = torch.cat(data, dim=0)\n",
    "\n",
    "num_mixture = [2, 3, 4, 8, 10, 12, 14, 16]\n",
    "nmi=[]\n",
    "\n",
    "for num_components in num_mixture:\n",
    "    print(f\"Training GMM with {num_components} components...\")\n",
    "\n",
    "    # Create GMM model and train\n",
    "    gmm_model = GMM(num_components=num_components, num_iterations=40, tolerance=1e-4)\n",
    "    gmm_model.train(data)\n",
    "\n",
    "    # Plot likelihood curve\n",
    "    gmm_model.plot_likelihood_curve()\n",
    "\n",
    "    # Generate and visualize samples:\n",
    "    gmm_model.visualize_samples(100)\n",
    "\n",
    "    cluster_assignments = gmm_model.E_Step(data).argmax(dim=1).cpu()\n",
    "    true_labels = torch.tensor(train_dataset.targets) \n",
    "\n",
    "    #Calculate NMI\n",
    "    nmi_score = gmm_model.calculate_nmi(true_labels, cluster_assignments)\n",
    "    nmi.append(nmi_score)\n",
    "    print(f\"NMI for {num_components} components: {nmi_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_mixture,nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc020b01",
   "metadata": {},
   "source": [
    "* A GMM class is defined, which has methods for initializing model parameters, performing the Expectation-Maximization (E-Step and M-Step) for training, computing the likelihood of data given the model, training the model, generating samples, and visualizing the samples.\n",
    "\n",
    "* The code is set up to train the GMM on a dataset of images. It loads and preprocesses images using the transforms.Compose function, and creates a DataLoader for the training dataset. The images are flattened to create feature vectors for GMM training.\n",
    "\n",
    "* The GMM is trained with different numbers of mixture components (num_mixture) ranging from 2 to 16. For each number of components, the GMM is trained, the likelihood curve is plotted, and samples are generated and visualized.\n",
    "\n",
    " * The code calculates the Normalized Mutual Information (NMI) between the true class labels and the cluster assignments obtained from the GMM. NMI is used to evaluate the quality of clustering.\n",
    "\n",
    "* The NMI scores for different numbers of mixture components are stored in the nmi list for further analysis. And likelihood curves and generated samples shown above.\n",
    "\n",
    "* NMI scores are very less due to size of image 16 x 16(due to computation limitation), due to the small size it can't able to detect object clearly and classifying most of images in one cluster.\n",
    "\n",
    "* NMI scores for the mixture components [2, 3, 4, 8, 10, 12, 14, 16] are [0.00010338137508369982, 0.00023964836145751178, 0.00034696151851676404,0.0014466078719124198, 0.0012399664847180247, 8.245532080763951e-05,0.00047308814828284085, 8.962965512182564e-05]\n",
    "\n",
    "* It was observed that increasing size of the image like 32 x 32, there is chances of cov-matrix is no more semi positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710cb51",
   "metadata": {},
   "source": [
    "# Question 2: Vanilla VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 4, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Conv2d(4, 8, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )  # output shape(Batch Size, 64, 4, 4)\n",
    "        \n",
    "        # Separate linear layers for mean and log-variance\n",
    "        self.fc_mu = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "        \n",
    "        #self.z_back = torch.nn.Linear(latent_dim, 64 * 4 *4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64 * 4 * 4),\n",
    "            nn.Unflatten(1, (64, 4, 4)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ConvTranspose2d(8, 4, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4),            \n",
    "            nn.ConvTranspose2d(4, input_channels, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.Sigmoid()  # Sigmoid activation for pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        enc_output = self.encoder(x)\n",
    "#        print('Encoder', enc_output.shape)\n",
    "        enc_output = enc_output.view(enc_output.size(0), -1)  # Flatten the output\n",
    "#        print('Encoder', enc_output.shape)\n",
    "        mu = self.fc_mu(enc_output)\n",
    "        log_var = self.fc_log_var(enc_output)\n",
    "#         print('mu',mu.shape)\n",
    "#         print('logvar',log_var.shape)\n",
    "        # Reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "         \n",
    "#        print(z.shape)\n",
    "        # Decode\n",
    "        dec_output = self.decoder(z)  # Reshape z for convolutional decoder\n",
    "        return dec_output, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fe04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAELoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, x_recon, mu, logvar, latent_dim, image_size):\n",
    "        # Calculate the likelihood loss (reconstruction loss) using MSE\n",
    "        likelihood_loss = nn.functional.mse_loss(x_recon, x)\n",
    "\n",
    "        # Calculate the KL divergence loss using torch.distributions.kl_divergence\n",
    "        # Define the prior distribution as a standard normal (mean=0, std=1)\n",
    "        prior = torch.distributions.Normal(0, 1)\n",
    "        \n",
    "        # Define the variational posterior distribution based on mu and logvar\n",
    "        posterior = torch.distributions.Normal(mu, logvar.exp().sqrt())\n",
    "        \n",
    "        # Calculate the KL divergence between the posterior and the prior\n",
    "        kl_loss = torch.mean(torch.distributions.kl.kl_divergence(posterior, prior))\n",
    "        kl_loss= kl_loss * (latent_dim / (3 * image_size * image_size))\n",
    "\n",
    "        # Total loss is the sum of likelihood loss and KL loss\n",
    "        total_loss = likelihood_loss + kl_loss\n",
    "\n",
    "        return total_loss, likelihood_loss, kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7778a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "latent_dim = 32\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "input_dim = 3\n",
    "image_size = 128\n",
    "num_samples_z = [1,3,5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b854277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset and other hyperparameters\n",
    "data_dir_train = 'afhq/train/' \n",
    "data_dir_val = 'afhq/val/'\n",
    "shuffle_dataset = True  \n",
    "num_workers = 4  \n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)), \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root='afhq/train/', transform=transform)\n",
    "val_dataset = ImageFolder(root='afhq/val/', transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=shuffle_dataset,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a820452",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(input_dim, latent_dim)  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "loss_fn = VAELoss()\n",
    "vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(total_losses, likelihood_losses, kl_losses, num):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Training losses\n",
    "    plt.plot(range(1, len(total_losses) + 1), total_losses, label='Train Total Loss', linestyle='--', color='blue')\n",
    "    plt.plot(range(1, len(likelihood_losses) + 1), likelihood_losses, label='Train Reconstruction Loss', linestyle='--', color='green')\n",
    "    plt.plot(range(1, len(kl_losses) + 1), kl_losses, label='Train KL Divergence Loss', linestyle='--', color='red')\n",
    "\n",
    "    # Validation losses\n",
    "    plt.plot(range(1, len(val_total_losses) + 1), val_total_losses, label='Validation Total Loss', color='blue')\n",
    "    plt.plot(range(1, len(val_likelihood_losses) + 1), val_likelihood_losses, label='Validation Reconstruction Loss', color='green')\n",
    "    plt.plot(range(1, len(val_kl_losses) + 1), val_kl_losses, label='Validation KL Divergence Loss', color='red')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Loss Curves (Training & Validation)  for num_sample_z = {num}')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a grid of original and reconstructed images\n",
    "def plot_original_reconstructed(vae, dataloader, device, num, num_samples=100):\n",
    "    vae.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Sample random images from the validation dataset\n",
    "        data_iter = iter(dataloader)\n",
    "        images, _ = next(data_iter)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Reconstruct the sampled images\n",
    "        reconstructed_images, _, _ = vae(images)\n",
    "        \n",
    "        # Create two separate grids for original and reconstructed images\n",
    "        original_grid = vutils.make_grid(images[:num_samples], nrow=10, padding=2, normalize=True)\n",
    "        reconstructed_grid = vutils.make_grid(reconstructed_images[:num_samples], nrow=10, padding=2, normalize=True)\n",
    "        \n",
    "        # Combine original and reconstructed grids side by side\n",
    "        combined_grid = torch.cat([original_grid, reconstructed_grid], dim=2)\n",
    "        \n",
    "        # Plot the combined grid of images\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(combined_grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Original (Left) vs. Reconstructed (Right) Images for num_sample_z = {num}')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate random images\n",
    "def generate_images(vae, device, num, num_samples=100):\n",
    "    vae.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Generate random samples from the latent space\n",
    "        latent_samples = torch.randn(num_samples, latent_dim).to(device)\n",
    "        \n",
    "        # Pass the samples through the decoder to generate images\n",
    "        generated_images = vae.decoder(latent_samples)\n",
    "        \n",
    "        # Create a grid of generated images\n",
    "        generated_grid = vutils.make_grid(generated_images, nrow=10, padding=2, normalize=True)\n",
    "        \n",
    "        # Plot the grid of generated images\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(generated_grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Generated Images for num_sample_z = {num}')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c57ab1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for num in num_samples_z:\n",
    "    # Initialize lists to store loss values for plotting\n",
    "    total_losses = []\n",
    "    likelihood_losses = []\n",
    "    kl_losses = []\n",
    "\n",
    "    early_stopping_patience = 5\n",
    "    epochs_without_improvement = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Lists to store validation losses\n",
    "    val_total_losses = []\n",
    "    val_likelihood_losses = []\n",
    "    val_kl_losses = []\n",
    "\n",
    "    # Training loop with tqdm\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        likelihood_loss_epoch = 0.0\n",
    "        kl_loss_epoch = 0.0\n",
    "\n",
    "        # Use tqdm to create a progress bar for the training loop\n",
    "        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            x,_ = batch\n",
    "\n",
    "\n",
    "            batch_likelihood_loss = 0\n",
    "            batch_kl_loss = 0\n",
    "            _, mu, logvar = vae(x)\n",
    "\n",
    "            for _ in range(num):\n",
    "    #             x_recon, mu, logvar = vae(x)\n",
    "                z = vae.reparameterize(mu, logvar)\n",
    "                x_recon = vae.decoder(z)\n",
    "                _, likelihood_loss, kl_loss = loss_fn(x, x_recon, mu, logvar, latent_dim, image_size)\n",
    "                batch_likelihood_loss += likelihood_loss\n",
    "                batch_kl_loss += kl_loss        \n",
    "\n",
    "            batch_likelihood_loss /= num\n",
    "            batch_kl_loss /= num\n",
    "            total_loss = batch_likelihood_loss + batch_kl_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update loss values for the current epoch\n",
    "            total_loss_epoch += total_loss.item()\n",
    "            likelihood_loss_epoch += likelihood_loss.item()\n",
    "            kl_loss_epoch += kl_loss.item()\n",
    "\n",
    "            # Update the progress bar description\n",
    "            progress_bar.set_description(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "\n",
    "        # Calculate and store the average loss for the epoch\n",
    "        avg_total_loss = total_loss_epoch / len(train_dataloader)\n",
    "        avg_likelihood_loss = likelihood_loss_epoch / len(train_dataloader)\n",
    "        avg_kl_loss = kl_loss_epoch / len(train_dataloader)\n",
    "\n",
    "        total_losses.append(avg_total_loss)\n",
    "        likelihood_losses.append(avg_likelihood_loss)\n",
    "        kl_losses.append(avg_kl_loss)\n",
    "\n",
    "        print(f'Total Loss: {avg_total_loss}, Likelihood Loss: {avg_likelihood_loss}, KL Loss: {avg_kl_loss}')\n",
    "         # Validation loop\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_epoch = 0.0\n",
    "            val_likelihood_loss_epoch = 0.0\n",
    "            val_kl_loss_epoch = 0.0\n",
    "\n",
    "            for batch in val_dataloader:\n",
    "                x, _ = batch\n",
    "                _, mu, logvar = vae(x)\n",
    "\n",
    "                val_batch_likelihood_loss = 0\n",
    "                val_batch_kl_loss = 0\n",
    "\n",
    "                for _ in range(num):\n",
    "                    z = vae.reparameterize(mu, logvar)\n",
    "                    x_recon = vae.decoder(z)\n",
    "                    _, likelihood_loss, kl_loss = loss_fn(x, x_recon, mu, logvar, latent_dim, image_size)\n",
    "\n",
    "                    val_batch_likelihood_loss += likelihood_loss\n",
    "                    val_batch_kl_loss += kl_loss\n",
    "\n",
    "                val_batch_likelihood_loss /= num\n",
    "                val_batch_kl_loss /= num\n",
    "                val_total_loss = val_batch_likelihood_loss + val_batch_kl_loss\n",
    "\n",
    "                val_loss_epoch += val_total_loss.item()\n",
    "                val_likelihood_loss_epoch += val_batch_likelihood_loss.item()\n",
    "                val_kl_loss_epoch += val_batch_kl_loss.item()\n",
    "\n",
    "            avg_val_total_loss = val_loss_epoch / len(val_dataloader)\n",
    "            avg_val_likelihood_loss = val_likelihood_loss_epoch / len(val_dataloader)\n",
    "            avg_val_kl_loss = val_kl_loss_epoch / len(val_dataloader)\n",
    "\n",
    "            val_total_losses.append(avg_val_total_loss)\n",
    "            val_likelihood_losses.append(avg_val_likelihood_loss)\n",
    "            val_kl_losses.append(avg_val_kl_loss)\n",
    "\n",
    "            print(f'Validation - Total Loss: {avg_val_total_loss}, Likelihood Loss: {avg_val_likelihood_loss}, KL Loss: {avg_val_kl_loss}')\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val_total_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_total_loss\n",
    "                epochs_without_improvement = 0\n",
    "                torch.save(vae.state_dict(), 'best_model_weights.pth') # save the model weights\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            if epochs_without_improvement >= early_stopping_patience:\n",
    "                print(f'Early stopping after {early_stopping_patience} epochs without improvement.')\n",
    "                break\n",
    "\n",
    "        vae.train()  # Set the model back to training mode at the end of the validation loop\n",
    "\n",
    "        \n",
    "    #Plottingof Total loss, Likelihood loss and KL loss\n",
    "    plot(total_losses, likelihood_losses, kl_losses, num)\n",
    "    # Plot original and reconstructed images\n",
    "    plot_original_reconstructed(vae, val_dataloader, device, num)\n",
    "    # Generate and plot new images\n",
    "    generate_images(vae, device, num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a9750",
   "metadata": {},
   "source": [
    "* VAE Architecture and Loss Function: The VAE architecture is defined in the VAE class. It consists of an encoder and a decoder.The VAELoss class defines the custom loss function for training the VAE. It includes a reconstruction loss (mean squared error) and a KL divergence loss to regularize the latent space.\n",
    "\n",
    "* Hyperparameters: Key hyperparameters such as latent_dim, learning_rate, batch_size, num_epochs, input_dim, and image_size are set. num_samples_z is a list containing the different numbers of samples drawn from the latent space (z) that will be experimented with.\n",
    "\n",
    "* Data Preparation: The code prepares the dataset for training the VAE using PyTorch's data loading utilities, such as ImageFolder for loading image data and creating data loaders.\n",
    "\n",
    "* Training Loop: The code runs a training loop for each value of num_samples_z. Within the loop, the VAE is trained for a specified number of epochs. During each training epoch, the code computes the total loss, likelihood loss, and KL divergence loss for both the training and validation datasets. The training loop also includes early stopping, which monitors the validation loss and stops training if no improvement is observed after a certain number of epochs.\n",
    "\n",
    "* Plotting and Visualization: After training, the code generates plots to visualize the training and validation losses for each experiment. It also generates and plots original and reconstructed images to assess the quality of the VAE's reconstructions. Finally, it generates new images from the VAE and plots them to see how well the model can generate novel data.\n",
    "\n",
    "* We are carefully adjusting the model's architecture and training process to mitigate the risk of gradients becoming NaN values during training. This includes strategies to ensure numerical stability, such as gradient clipping, proper weight initialization, learning rate scheduling, and regularization techniques. These measures help us prevent numerical instability and maintain a stable training process for the VAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849e3e1",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN-based classifier model using nn.Sequential\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 16 * 16, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 64 * 16 * 16)  \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_classes = len(np.unique(train_dataset.targets))   \n",
    "# Initialize the classifier model and optimizer\n",
    "classifier = CNNClassifier(num_classes)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_loss = float('inf')  \n",
    "patience = 5  \n",
    "counter = 0  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_data, batch_labels in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        outputs = classifier(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation loop\n",
    "    classifier.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in val_dataloader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            outputs = classifier(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += batch_labels.size(0)\n",
    "            correct_predictions += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "    # Calculate average validation loss for this epoch\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = (correct_predictions / total_predictions) * 100.0\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print training and validation losses and accuracy for this epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Implement early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save the best model's state\n",
    "        best_model_state = classifier.state_dict()\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1} due to no improvement in validation loss.')\n",
    "            break\n",
    "\n",
    "# Restore the best model's state\n",
    "classifier.load_state_dict(best_model_state)\n",
    "\n",
    "# After training, you can access the final validation accuracy using val_accuracies[-1]\n",
    "final_val_accuracy = val_accuracies[-1]\n",
    "print(f'Final Validation Accuracy: {final_val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502691c1",
   "metadata": {},
   "source": [
    "#### CNN-based Classifier:\n",
    "\n",
    "#### Advantages:\n",
    "* Achieves a high final validation accuracy (96.20%).\n",
    "* Effective in extracting image features directly from raw data.\n",
    "#### Considerations:\n",
    "* Larger model size and computation requirements.\n",
    "* May require a larger dataset for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440185b",
   "metadata": {},
   "source": [
    "### Posterior Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_vector(vae, dataloader, device):\n",
    "    vae.eval()  \n",
    "    latent_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels_batch in dataloader:\n",
    "            images = images.to(device)\n",
    "            _, mu, log_var = vae(images)  # Get mu and log_var\n",
    "            z = vae.reparameterize(mu, log_var)  # Sample latent vectors\n",
    "            latent_vectors.append(z.cpu().numpy())\n",
    "            labels.append(labels_batch.numpy())\n",
    "    \n",
    "    # Concatenate the latent vectors and labels\n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    return latent_vectors, labels\n",
    "\n",
    "train_latent_vectors, train_labels = get_latent_vector(vae, train_dataloader, device)\n",
    "val_latent_vectors, val_labels = get_latent_vector(vae, val_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_latent_vectors.shape, train_labels.shape, val_latent_vectors.shape,  val_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c04195",
   "metadata": {},
   "source": [
    "### MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        self.mlp_classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),  \n",
    "            nn.Linear(256, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),  \n",
    "            nn.Linear(128, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),  \n",
    "            nn.Linear(64, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),  \n",
    "            nn.Linear(32, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),  \n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp_classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "num_classes = len(np.unique(train_dataset.targets))   \n",
    "mlp_cls = MLPClassifier(latent_dim, num_classes)\n",
    "mlp_cls.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_cls.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Define your early stopping criteria\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "train_dataloader_length = len(train_labels)\n",
    "progress_bar = tqdm(range(num_epochs), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in progress_bar:\n",
    "    mlp_cls.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    train_dataset1 = torch.utils.data.TensorDataset(torch.tensor(train_latent_vectors, dtype=torch.float32), torch.tensor(train_labels, dtype=torch.long))\n",
    "    train_loader1 = torch.utils.data.DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for inputs, labels in train_loader1:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_cls(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    average_train_loss = total_train_loss / len(train_loader1)\n",
    "\n",
    "    # Validation\n",
    "    mlp_cls.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_dataset1 = torch.utils.data.TensorDataset(torch.tensor(val_latent_vectors, dtype=torch.float32), torch.tensor(val_labels, dtype=torch.long))\n",
    "        val_loader1 = torch.utils.data.DataLoader(val_dataset1, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        for inputs, labels in val_loader1:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            val_outputs = mlp_cls(inputs)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss for this epoch\n",
    "    average_val_loss = total_val_loss / len(val_loader1)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    # Print training and validation statistics using tqdm\n",
    "    progress_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Val Loss: {average_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Calculate final accuracy on validation data\n",
    "final_val_accuracy = val_accuracy\n",
    "\n",
    "print(f\"Final Validation Accuracy: {final_val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0e9eb",
   "metadata": {},
   "source": [
    "#### MLP-based Classifier with Latent Vectors:\n",
    "\n",
    "#### Advantages:\n",
    "* Achieves a respectable validation accuracy (81.20%).\n",
    "* Smaller model size, computationally efficient.\n",
    "* Effective transfer learning from VAE latent vectors.\n",
    "#### Considerations:\n",
    "* Lower accuracy compared to the CNN-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'q2_vanila_vae.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dee949",
   "metadata": {},
   "source": [
    "### Question 3: Beta VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183ad95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "beta = [.01, .1, .5, .8, .95, 1, 1.05, 1.5, 5, 10, 100, 500 ]\n",
    "num_samples_z = 1\n",
    "\n",
    "num_epochs = 10\n",
    "for i in beta:\n",
    "    vae = VAE(input_dim, latent_dim)  \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    loss_fn = VAELoss()\n",
    "    vae.to(device)\n",
    "    \n",
    "    # Initialize lists to store loss values for plotting\n",
    "    total_losses = []\n",
    "    likelihood_losses = []\n",
    "    kl_losses = []\n",
    "\n",
    "    # Training loop with tqdm\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        likelihood_loss_epoch = 0.0\n",
    "        kl_loss_epoch = 0.0\n",
    "\n",
    "        # Use tqdm to create a progress bar for the training loop\n",
    "        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            x,_ = batch\n",
    "        \n",
    "        \n",
    "            batch_likelihood_loss = 0\n",
    "            batch_kl_loss = 0\n",
    "        \n",
    "            for _ in range(num_samples_z):\n",
    "                x_recon, mu, logvar = vae(x)\n",
    "                _, likelihood_loss, kl_loss = loss_fn(x, x_recon, mu, logvar, latent_dim, image_size)\n",
    "                batch_likelihood_loss += likelihood_loss\n",
    "                batch_kl_loss += kl_loss        \n",
    "        \n",
    "            batch_likelihood_loss /= num_samples_z\n",
    "            batch_kl_loss /= num_samples_z\n",
    "            total_loss = batch_likelihood_loss + i * batch_kl_loss\n",
    "        \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update loss values for the current epoch\n",
    "            total_loss_epoch += total_loss.item()\n",
    "            likelihood_loss_epoch += likelihood_loss.item()\n",
    "            kl_loss_epoch += kl_loss.item()\n",
    "\n",
    "            # Update the progress bar description\n",
    "            progress_bar.set_description(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "\n",
    "        # Calculate and store the average loss for the epoch\n",
    "        avg_total_loss = total_loss_epoch / len(train_dataloader)\n",
    "        avg_likelihood_loss = likelihood_loss_epoch / len(train_dataloader)\n",
    "        avg_kl_loss = kl_loss_epoch / len(train_dataloader)\n",
    "\n",
    "        total_losses.append(avg_total_loss)\n",
    "        likelihood_losses.append(avg_likelihood_loss)\n",
    "        kl_losses.append(avg_kl_loss)\n",
    "    \n",
    "        print(f'Total Loss: {avg_total_loss}, Likelihood Loss: {avg_likelihood_loss}, KL Loss: {avg_kl_loss}')\n",
    "    \n",
    "\n",
    "   \n",
    "    # Function to generate a grid of original and reconstructed images\n",
    "    def plot_original_reconstructed(vae, dataloader, device, num_samples=100):\n",
    "        vae.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            # Sample random images from the validation dataset\n",
    "            data_iter = iter(dataloader)\n",
    "            images, _ = next(data_iter)\n",
    "            images = images.to(device)\n",
    "        \n",
    "            # Reconstruct the sampled images\n",
    "            reconstructed_images, _, _ = vae(images)\n",
    "        \n",
    "            # Create two separate grids for original and reconstructed images\n",
    "            original_grid = vutils.make_grid(images[:num_samples], nrow=10, padding=2, normalize=True)\n",
    "            reconstructed_grid = vutils.make_grid(reconstructed_images[:num_samples], nrow=10, padding=2, normalize=True)\n",
    "        \n",
    "            # Combine original and reconstructed grids side by side\n",
    "            combined_grid = torch.cat([original_grid, reconstructed_grid], dim=2)\n",
    "        \n",
    "            # Plot the combined grid of images\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.imshow(combined_grid.permute(1, 2, 0).cpu().numpy())\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Original (Left) vs. Reconstructed (Right) Images for beta =  {i}')\n",
    "            plt.show()\n",
    "\n",
    "    # Plot original and reconstructed images\n",
    "    plot_original_reconstructed(vae, val_dataloader, device)\n",
    "\n",
    "    # Function to generate random images\n",
    "    def generate_images(vae, device, num_samples=100):\n",
    "        vae.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            # Generate random samples from the latent space\n",
    "            latent_samples = torch.randn(num_samples, latent_dim).to(device)\n",
    "        \n",
    "            # Pass the samples through the decoder to generate images\n",
    "            generated_images = vae.decoder(latent_samples)\n",
    "        \n",
    "            # Create a grid of generated images\n",
    "            generated_grid = vutils.make_grid(generated_images, nrow=10, padding=2, normalize=True)\n",
    "        \n",
    "            # Plot the grid of generated images\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(generated_grid.permute(1, 2, 0).cpu().numpy())\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Generated Images for beta = {i}')\n",
    "            plt.show()\n",
    "\n",
    "    # Generate and plot new images\n",
    "    generate_images(vae, device)\n",
    "    filename = f'q2_vanilla_vae {i}.pth'\n",
    "    torch.save(vae.state_dict(), filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb1d34",
   "metadata": {},
   "source": [
    "#### Effect of Beta on Training:\n",
    "\n",
    "* As \"beta\" increases, the weight of the KL divergence term in the VAE loss becomes more significant relative to the reconstruction loss. This encourages the VAE to have a more structured latent space, which may lead to better-organized representations of data.\n",
    "\n",
    "#### Impact on Reconstruction:\n",
    "\n",
    "* Lower values of \"beta\" (e.g., 0.01, 0.1) result in less emphasis on the KL divergence term. As a result, the VAE may prioritize accurate image reconstruction over enforcing a structured latent space. This can lead to reconstructions that are closer to the original data, but the latent space may not be well-organized.\n",
    "\n",
    "* Higher values of \"beta\" (e.g., 5, 10, 100, 500) give more weight to the KL divergence term. This encourages the VAE to have a more structured latent space but can lead to reconstructions that may deviate from the original data as the model seeks to impose a more defined structure on the latent space.\n",
    "\n",
    "#### Effect on Image Generation:\n",
    "\n",
    "* Low \"beta\" values (e.g., 0.01, 0.1) may produce generated images that closely resemble training data but exhibit less variety.\n",
    "\n",
    "* Higher \"beta\" values (e.g., 5, 10, 100, 500) may result in generated images with more diversity, but they might not be as faithful to the original training data. The model prioritizes exploring the structured latent space, which can lead to novel but less realistic image samples.\n",
    "\n",
    "In summary, adjusting the \"beta\" hyperparameter in a VAE allows you to control the trade-off between reconstruction quality and the structure of the latent space. Lower \"beta\" values prioritize reconstruction fidelity, while higher \"beta\" values encourage a more organized latent space and diverse image generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602244b1",
   "metadata": {},
   "source": [
    "#### Posterior Inference for Beta = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(input_dim, latent_dim)  \n",
    "vae.load_state_dict(torch.load('q2_vanilla_vae 0.8.pth'))\n",
    "vae.eval()\n",
    "num_interpolations = 10\n",
    "num_pairs = 10\n",
    "pair_indices = torch.randint(0, len(val_dataset), (num_pairs, 2))\n",
    "for pair_index in pair_indices:\n",
    "    # Choose two images from the dataset\n",
    "    image1 = val_dataset[pair_index[0]]\n",
    "    image2 = val_dataset[pair_index[1]]\n",
    "\n",
    "    # Encode the images to get their latent representations\n",
    "    with torch.no_grad():\n",
    "        _, mu1, log_var1 = vae(image1[0].unsqueeze(0))\n",
    "        _, mu2, log_var2 = vae(image2[0].unsqueeze(0))\n",
    "        latent1 = vae.reparameterize(mu1, log_var1)\n",
    "        latent2 = vae.reparameterize(mu2, log_var2)\n",
    "\n",
    "    # Interpolate between the latent representations\n",
    "    interpolated_latents = []\n",
    "    for alpha in torch.linspace(0, 1, num_interpolations):\n",
    "        interpolated_latent = alpha * latent1 + (1 - alpha) * latent2\n",
    "        interpolated_latents.append(interpolated_latent)\n",
    "\n",
    "    # Generate images from interpolated latents\n",
    "    with torch.no_grad():\n",
    "        interpolated_images = vae.decoder(torch.stack(interpolated_latents).view(10,32))\n",
    "\n",
    "    # Plot the interpolated images\n",
    "    plt.figure(figsize=(15, 1.5))\n",
    "    for i, img in enumerate(interpolated_images):\n",
    "        plt.subplot(1, num_interpolations, i + 1)\n",
    "        plt.imshow(img.permute(1, 2, 0).cpu().numpy())  # Convert to numpy and permute the dimensions\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "    plt.suptitle(f'Interpolation between Pair {pair_index[0]} and Pair {pair_index[1]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b744ed9",
   "metadata": {},
   "source": [
    "* The code provides a visual representation of the VAE's latent space by interpolating between the latent representations of pairs of images and generating images at various points along the interpolation path. \n",
    "* This demonstrates the VAE's ability to capture and manipulate image features in a structured and continuous latent space. \n",
    "* The results show that the VAE can smoothly transition between images, indicating its effectiveness in encoding and decoding visual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa4c20",
   "metadata": {},
   "source": [
    "### Question 4: Vanila Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanila_AE(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(Vanila_AE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 4, kernel_size=4, stride=2, padding=1),  # Convolutional layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Conv2d(4, 8, kernel_size=4, stride=2, padding=1),  # Convolutional layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1),  # Convolutional layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),  # Convolutional layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # Convolutional layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "        )  # output shape(Batch Size,latent_dim)\n",
    "        \n",
    "        \n",
    "        self.fc_z = nn.Linear(64 * 4 * 4, latent_dim)       \n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64 * 4 * 4),\n",
    "            nn.Unflatten(1, (64, 4, 4)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Transposed convolutional layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # Transposed convolutional layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),  # Transposed convolutional layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ConvTranspose2d(8, 4, kernel_size=4, stride=2, padding=1),  # Transposed convolutional layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4),            \n",
    "            nn.ConvTranspose2d(4, input_channels, kernel_size=4, stride=2, padding=1),  # Transposed convolutional layer 2\n",
    "            nn.Sigmoid()  # Sigmoid activation for pixel values between 0 and 1\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "#         print(x.shape)\n",
    "        enc_output = self.encoder(x)\n",
    "#         print('Encoder', enc_output.shape)\n",
    "        enc_output = enc_output.view(enc_output.size(0), -1)  # Flatten the output\n",
    "#        print('Encoder', enc_output.shape)\n",
    "        z = self.fc_z(enc_output)\n",
    "\n",
    "#         print(z.shape)\n",
    "        # Decode\n",
    "        dec_output = self.decoder(z)  # Reshape z for convolutional decoder\n",
    "        return z, dec_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad96564",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae=Vanila_AE(input_dim, latent_dim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "optimizer = optim.Adam(ae.parameters(), lr=learning_rate)\n",
    "ae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4161f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store loss values for plotting\n",
    "total_losses = []\n",
    "likelihood_losses = []\n",
    "kl_losses = []\n",
    "num_epochs = 10\n",
    "# Training loop with tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_epoch = 0.0\n",
    "#     likelihood_loss_epoch = 0.0\n",
    "#     kl_loss_epoch = 0.0\n",
    "\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        x,_ = batch\n",
    "        \n",
    "        \n",
    "        batch_likelihood_loss = 0\n",
    "        batch_kl_loss = 0\n",
    "        \n",
    "\n",
    "        latent, x_recon = ae(x)\n",
    "        total_loss = nn.functional.mse_loss(x_recon, x)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ae.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss values for the current epoch\n",
    "        total_loss_epoch += total_loss.item()\n",
    "\n",
    "        # Update the progress bar description\n",
    "        progress_bar.set_description(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "\n",
    "    # Calculate and store the average loss for the epoch\n",
    "    avg_total_loss = total_loss_epoch / len(train_dataloader)\n",
    "\n",
    "    total_losses.append(avg_total_loss)\n",
    "\n",
    "    print(f'Total Loss: {avg_total_loss}')#, Likelihood Loss: {avg_likelihood_loss}, KL Loss: {avg_kl_loss}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), total_losses, label='Total Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2faf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'q4_vanilla_ae.pth'\n",
    "torch.save(ae.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a grid of original and reconstructed images\n",
    "def plot_original_reconstructed(ae, dataloader, device, num_samples=100):\n",
    "    ae.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Sample random images from the validation dataset\n",
    "        data_iter = iter(dataloader)\n",
    "        images, _ = next(data_iter)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Reconstruct the sampled images\n",
    "        _, reconstructed_images = ae(images)\n",
    "        \n",
    "        # Create two separate grids for original and reconstructed images\n",
    "        original_grid = vutils.make_grid(images[:num_samples], nrow=10, padding=2, normalize=True)\n",
    "        reconstructed_grid = vutils.make_grid(reconstructed_images[:num_samples], nrow=10, padding=2, normalize=True)\n",
    "        \n",
    "        # Combine original and reconstructed grids side by side\n",
    "        combined_grid = torch.cat([original_grid, reconstructed_grid], dim=2)\n",
    "        \n",
    "        # Plot the combined grid of images\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(combined_grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title('Original (Left) vs. Reconstructed (Right) Images')\n",
    "        plt.show()\n",
    "\n",
    "# Plot original and reconstructed images\n",
    "plot_original_reconstructed(ae, val_dataloader, device)\n",
    "\n",
    "# Function to generate random images\n",
    "def generate_images(ae, device, num_samples=100):\n",
    "    ae.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Generate random samples from the latent space\n",
    "        latent_samples = torch.randn(num_samples, latent_dim).to(device)\n",
    "        \n",
    "        # Pass the samples through the decoder to generate images\n",
    "        generated_images = ae.decoder(latent_samples)\n",
    "        \n",
    "        # Create a grid of generated images\n",
    "        generated_grid = vutils.make_grid(generated_images, nrow=10, padding=2, normalize=True)\n",
    "        \n",
    "        # Plot the grid of generated images\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(generated_grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title('Generated Images')\n",
    "        plt.show()\n",
    "\n",
    "# Generate and plot new images\n",
    "generate_images(ae, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a23f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_vector_ae(ae, dataloader, device):\n",
    "    ae.eval()  \n",
    "    latent_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels_batch in dataloader:\n",
    "            images = images.to(device)\n",
    "            z,_ = ae(images)  # Get latent vectors\n",
    "#             z = vae.reparameterize(mu, log_var)  # Sample \n",
    "            latent_vectors.append(z.cpu().numpy())\n",
    "            labels.append(labels_batch.numpy())\n",
    "    \n",
    "    # Concatenate the latent vectors and labels\n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    return latent_vectors, labels\n",
    "\n",
    "train_latent_vectors, train_labels = get_latent_vector_ae(ae, train_dataloader, device)\n",
    "val_latent_vectors, val_labels = get_latent_vector_ae(ae, val_dataloader, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        self.mlp_classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),  \n",
    "            nn.Linear(256, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),  \n",
    "            nn.Linear(128, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),  \n",
    "            nn.Linear(64, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),  \n",
    "            nn.Linear(32, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),  \n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp_classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "num_classes = len(np.unique(train_dataset.targets))   \n",
    "mlp_cls = MLPClassifier(latent_dim, num_classes)\n",
    "mlp_cls.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_cls.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Define your early stopping criteria\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "train_dataloader_length = len(train_labels)\n",
    "progress_bar = tqdm(range(num_epochs), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in progress_bar:\n",
    "    mlp_cls.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    train_dataset1 = torch.utils.data.TensorDataset(torch.tensor(train_latent_vectors, dtype=torch.float32), torch.tensor(train_labels, dtype=torch.long))\n",
    "    train_loader1 = torch.utils.data.DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for inputs, labels in train_loader1:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_cls(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    average_train_loss = total_train_loss / len(train_loader1)\n",
    "\n",
    "    # Validation\n",
    "    mlp_cls.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_dataset1 = torch.utils.data.TensorDataset(torch.tensor(val_latent_vectors, dtype=torch.float32), torch.tensor(val_labels, dtype=torch.long))\n",
    "        val_loader1 = torch.utils.data.DataLoader(val_dataset1, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        for inputs, labels in val_loader1:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            val_outputs = mlp_cls(inputs)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss for this epoch\n",
    "    average_val_loss = total_val_loss / len(val_loader1)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    # Print training and validation statistics using tqdm\n",
    "    progress_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Val Loss: {average_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Calculate final accuracy on validation data\n",
    "final_val_accuracy = val_accuracy\n",
    "\n",
    "print(f\"Final Validation Accuracy: {final_val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a84ff",
   "metadata": {},
   "source": [
    "#### Comparing accuracy with VAE \n",
    "* Accuracy with AE = 81.00% and VAE = 81.20%\n",
    "* This similarity could be attributed to the quality of the features extracted by the encoder part of both AE and VAE.\n",
    "* It's possible that the latent spaces learned by both models capture similar underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbe1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_latent_vectors = torch.from_numpy(train_latent_vectors).float().to(device)\n",
    "val_latent_vectors = torch.from_numpy(val_latent_vectors).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_on_latents = GMM(num_components=10, num_iterations=40, tolerance=1e-4)\n",
    "gmm_on_latents.train(train_latent_vectors)\n",
    "# Plot likelihood curve\n",
    "gmm_on_latents.plot_likelihood_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=100\n",
    "gmm_on_latents.generate_samples(num_samples).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_latents = gmm_on_latents.generate_samples(100)\n",
    "with torch.no_grad():\n",
    "    reconstructed_images = ae.decoder(new_latents)\n",
    "\n",
    "reconstructed_grid = vutils.make_grid(reconstructed_images[:num_samples], nrow=10, padding=2, normalize=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(reconstructed_grid.permute(1, 2, 0).cpu().numpy())  # Using reconstructed_grid instead of reconstructed_images\n",
    "plt.axis('off')\n",
    "plt.title('GMM parameter for AE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f230c37",
   "metadata": {},
   "source": [
    "* It is difficult to compare the results with GMM because image obtained from is of size 16 x 16 and image obtained from AE is 128 x 128."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
